<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-dEjpd4rt1TGFt3jkg1KJYw" name="new_supporting_material,_d4L8wCrUEd6FjITr_tW4FQ" guid="-dEjpd4rt1TGFt3jkg1KJYw" changeDate="2010-10-20T09:25:51.518-0700" version="7.5.0">
  <mainDescription>&lt;h3>&#xD;
    Purpose&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    This metric helps a team understand overall solution quality by studying two factors: test completeness and test&#xD;
    success. The test completion rate enables the team to manage what part of the test effort remains, including the risk&#xD;
    associated with potentially undiscovered quality issues. The test success rate helps the team to decide whether the&#xD;
    software or system is working correctly. This metric should be used in conjunction with the Defect Trends metric to&#xD;
    determine release readiness of the solution.&lt;br />&#xD;
    &lt;br />&#xD;
    When the term &quot;test&quot; is used in this metric, it refers to the executable tests that contain the steps needed to run&#xD;
    against the actual solution under development. These tests can be manual or automated. The Rational Unified Process&#xD;
    refers to these tests as &quot;test scripts&quot;, but in some test methodologies and test tools, these are referred to as test&#xD;
    cases. To avoid confusion, this metric uses the term &quot;test.&quot;&lt;br />&#xD;
    &lt;br />&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Definition&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Test Execution Status is measured by reporting the following items once per iteration and trending throughout the&#xD;
    release cycle:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Tests planned.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Tests implemented.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Tests attempted.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Passed tests.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Failed tests.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Blocked tests.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;br />&#xD;
    The term iteration is sometimes called &quot;test cycle,&quot; but these are not synonymous. A single iteration may have multiple&#xD;
    test cycles, based on the build cycle. The recommendation is to capture this metric once per iteration, regardless of&#xD;
    the number of test cycles.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Terms&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Tests planned = The number of tests scheduled to be executed in the iteration.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Tests implemented = The number of tests built and ready to be executed - both manually and automatically - in the&#xD;
    iteration.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Tests attempted = The number of tests that have been executed, and is the sum of the passed, failed and blocked tests.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Passed tests = The number of tests that have a most recent result of pass.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Failed tests = The number of tests that have a most recent result of failed.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Blocked tests = The number of tests that cannot be executed completely to the last step of the test. For manual tests,&#xD;
    this means the tester could not execute all the steps of the test. For automated tests, the automated testing tool&#xD;
    reports a passing result, but the human test analyst determines that the test was invalid using information outside the&#xD;
    scope of what the automated testing tool can report.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Build Health is captured in IBM&amp;reg; Rational&amp;reg; Team Concert&amp;reg; and BM&amp;reg; Rational&amp;reg; Insight&amp;reg;.&lt;br />&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Analysis&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Use either a line or bar graph that shows number of tests on the y-axis and the iterations along the x-axis. Categorize&#xD;
    the tests as indicated above. Ideally, show the results against a planned schedule for implementation and successful&#xD;
    execution. The following patterns might occur:&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Rising slope&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests planned, tests implemented, tests attempted, and passed tests:&lt;/strong> This is the desired trend.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Failed tests:&lt;/strong> Indicates decreasing solution quality and/or decreasing requirement quality.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Blocked tests:&lt;/strong> Indicates the test effort may be falling behind schedule and the quality of the&#xD;
        solution is more unknown. Also, may be an indicator of technical problems or test data problems in the test&#xD;
        environment.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;br />&#xD;
    &lt;strong>Falling slope&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests planned:&lt;/strong> Indicates tests are being removed from the scope of the test effort, possibly&#xD;
        indicating a decrease in overall scope for the release.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests implemented:&lt;/strong> Indicates there are not enough test resources to write the planned tests.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests attempted:&lt;/strong> Indicates there are not enough test resources to execute the planned and&#xD;
        implemented tests.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Passed tests:&lt;/strong> Depending on the trends of planned, implemented and attempted tests, this pattern&#xD;
        usually indicates decreasing solution quality, and/or previously passing tests are now failing.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Failed tests and blocked tests:&lt;/strong> This is the desired trend.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;br />&#xD;
    &lt;strong>Flat line&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests planned, tests implemented, and tests attempted:&lt;/strong> Indicates new tests are not being added to&#xD;
        the overall test effort for the release. Some root causes are: lack of test resources to implement and/or execute&#xD;
        tests, lack of clear requirements, no new requirements being delivered to test.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Passed tests:&lt;/strong> Indicates defects are not being corrected. Could also indicate a coincidental net&#xD;
        zero difference in the number of passing tests.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Failed tests and blocked tests:&lt;/strong> Indicates there is a lack of test resources to execute previously&#xD;
        failed or blocked tests, or defects are not being corrected, or a coincidental net zero difference in the number of&#xD;
        failing or blocked tests, or some combination of these issues. The test schedule may be in jeopardy in this&#xD;
        scenario.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;strong>Hockey stick&lt;br />&#xD;
    &lt;/strong>&lt;br />&#xD;
    A hockey stick trend gradually increases or decreases, then takes a sharp turn in the upward direction, typically late&#xD;
    in the release cycle. This means the project is experiencing surprises at a time when things should be routine.&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests planned, tests implemented, tests attempted:&lt;/strong> Indicates many new tests are being added to the&#xD;
        test effort, possibly due to: new requirements added to the scope of the project, previously ambiguous requirements&#xD;
        have been clarified, additional test resources added.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Passed tests:&lt;/strong> Indicates the following possible scenarios: defects are being corrected and verified&#xD;
        more quickly, requirements are more clear, additional test resources have been added, or some combination of these&#xD;
        items.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Failed tests and blocked tests:&lt;/strong> Indicates the following possible issues: new tests are failing or&#xD;
        blocked, previously passing tests are now failing or blocked, requirements are not clear, previously corrected&#xD;
        defects are recurring, new defects are being discovered.&lt;br />&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;strong>Reverse hockey stick&lt;/strong>&lt;br />&#xD;
    &lt;br />&#xD;
    A reverse hockey stick trend gradually increases or decreases, then takes a sharp turn in the downward direction,&#xD;
    typically late in the release cycle. This means the project is experiencing surprises at a time when things should be&#xD;
    routine.&lt;br />&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;strong>Tests planned, tests implemented, tests attempted:&lt;/strong> Indicates tests have been removed from the&#xD;
        scope of the project - perhaps due to removal of requirements.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Passed tests:&lt;/strong> Indicates that a large number of tests that were previously passing are suddenly no&#xD;
        longer passing. Possible sources for this issue are: previously passing tests are now failing and/or newly&#xD;
        delivered solution quality is decreasing.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;strong>Failed tests and blocked tests:&lt;/strong> Indicates the following possible scenarios: defects are being&#xD;
        corrected and verified more quickly, requirements are more clear, additional test resources have been added, or&#xD;
        some combination of these items.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;br />&#xD;
    &lt;strong>Perfect slope&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Perfect slope describes a trend where the actual data exactly matches the expected behavior for a metric. Sometimes&#xD;
    there are patterns in perfect slope situations that allow us to give specific advice on corrective action. For this&#xD;
    metric, other than investigating potential bad data, there is no specific advice.&lt;br />&#xD;
    &lt;br />&#xD;
    &lt;br />&#xD;
    The following graph shows an example of two of the test execution status report trends - passed tests and failed&#xD;
    tests.&lt;br />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;img alt=&quot;Test Execution Status&quot; src=&quot;./resources/test_execution_status.gif&quot; />&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Frequency and reporting&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Data is captured each day and monitored at the end of each iteration to help identify trends.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Collection and reporting tools&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Test execution data is captured in IBM&amp;reg; Rational&amp;reg; Quality Manager&amp;reg;. IBM&amp;reg; Rational&amp;reg; Insight&amp;reg; provides an out of the box&#xD;
    report focused on implemented tests and their status.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Assumptions and prerequisites&#xD;
&lt;/h3>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Test plans are updated as part of iteration planning and compared to actuals at the end of each iteration.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test execution data is captured in a tool, tracking the status of each attempted test.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h3>&#xD;
    Pitfalls, advice, and countermeasures for this metric&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The following items are indicators of measurement pitfalls and should be used to corroborate this metric:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Release size&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Requirement quality&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test capacity&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Defect aging&#xD;
    &lt;/li>&#xD;
&lt;/ul></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
