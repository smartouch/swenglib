<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-by_cXtkvdnZdgdRA5ifrcQ" name="key_measure_test,_AhYiUHHVEdyzS55ez-koKA" guid="-by_cXtkvdnZdgdRA5ifrcQ" version="7.1.0">
  <mainDescription>&lt;h3>&#xD;
    &lt;b>&lt;a id=&quot;Introduction&quot; name=&quot;Introduction&quot;>Introduction&lt;/a>&lt;/b>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The key measures of a test include coverage and quality.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Test coverage is the measurement of testing completeness, and it's based on the coverage of testing expressed by the&#xD;
    coverage of test requirements and test cases or by the coverage of executed code.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Quality is a measure of the reliability, stability, and performance of the target-of-test (system or&#xD;
    application-under-test). Quality is based on evaluating test results and analyzing change requests (defects) identified&#xD;
    during testing.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;b>&lt;a id=&quot;Coverage&quot; name=&quot;Coverage&quot;>Coverage Measures&lt;/a>&lt;/b>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Coverage metrics provide answers to the question: &quot;How complete is the testing?&quot; The most commonly-used measures of&#xD;
    coverage are based on the coverage of software requirements and source code. Basically, test coverage is any measure of&#xD;
    completeness with respect to either a requirement (requirement-based), or the code's design and implementation criteria&#xD;
    (code-based), such as verifying use cases (requirement-based) or executing all lines of code (code-based).&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Any systematic testing task is based on at least one test coverage strategy. The coverage strategy guides the design of&#xD;
    test cases by stating the general purpose of the testing. The statement of coverage strategy can be as simple as&#xD;
    verifying all performance.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A requirements-based coverage strategy might be sufficient for yielding a quantifiable measure of testing completeness&#xD;
    if the requirements are completely cataloged. For example, if all performance test requirements have been identified,&#xD;
    then the test results can be referenced to get measures; for example, 75% of the performance test requirements have&#xD;
    been verified.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    If code-based coverage is applied, test strategies are formulated in terms of how much of the source code has been&#xD;
    executed by tests. This type of test coverage strategy is very important for safety-critical systems.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Both measures can be derived manually (using the equations given in the next two headings) or may be calculated using&#xD;
    test automation tools.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Requirements-based test coverage&quot; name=&quot;Requirements-based test coverage&quot;>Requirements-based Test Coverage&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Requirements-based test coverage, measured several times during the test lifecycle, identifies the test coverage at a&#xD;
    milestone in the testing lifecycle, such as the planned, implemented, executed, and successful test coverage.&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Test coverage is calculated using the following equation:&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        Test Coverage = T&lt;sup>&lt;sup>(p,i,x,s)&lt;/sup>&lt;/sup> / RfT&lt;br />&#xD;
        &lt;br />&#xD;
        Where:&lt;br />&#xD;
        T is the number of Tests (planned, implemented, executed, or successful), expressed as test procedures or test&#xD;
        cases.&#xD;
    &lt;/p>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        RfT is the total number of Requirements for Test.&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Planning: the test coverage is calculated to determine the planned test coverage in the following manner:&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        Test Coverage (planned) = T&lt;sup>&lt;sup>p&lt;/sup>&lt;/sup> / RfT&lt;br />&#xD;
        &lt;br />&#xD;
        Where:&lt;br />&#xD;
        T&lt;sup>&lt;sup>p&lt;/sup>&lt;/sup> is the number of planned Tests, expressed as test procedures or test cases.&#xD;
    &lt;/p>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        RfT is the total number of Requirements for Test.&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Implementation: as test procedures are being implemented (as test scripts) test coverage is calculated using the&#xD;
        following equation:&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        Test Coverage (implemented) = T&lt;sup>&lt;sup>i&lt;/sup>&lt;/sup> / RfT&lt;br />&#xD;
        &lt;br />&#xD;
        Where:&lt;br />&#xD;
        T&lt;sup>&lt;sup>i&lt;/sup>&lt;/sup> is the number of Tests implemented, expressed by the number of test procedures or test&#xD;
        cases for which there are corresponding test scripts.&#xD;
    &lt;/p>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        RfT is the total number of Requirements for Test.&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Execution:&amp;nbsp;there are two test coverage measures used-one identifies the test coverage achieved by executing&#xD;
        the tests and the second identifies the successful test coverage (those tests that executed without failures, such&#xD;
        as defects or unexpected results). &#xD;
        &lt;p>&#xD;
            These coverage measures are calculated using the following equations:&#xD;
        &lt;/p>&#xD;
        &lt;blockquote>&#xD;
            &lt;p class=&quot;example&quot;>&#xD;
                Test Coverage (executed) = T&lt;sup>&lt;sup>x&lt;/sup>&lt;/sup> / RfT&#xD;
            &lt;/p>&#xD;
        &lt;/blockquote>&#xD;
        &lt;blockquote>&#xD;
            &lt;p class=&quot;example&quot;>&#xD;
                Where:&lt;br />&#xD;
                T&lt;sup>&lt;sup>x&lt;/sup>&lt;/sup> is the number of Tests executed, expressed as test procedures or test cases.&#xD;
            &lt;/p>&#xD;
        &lt;/blockquote>&#xD;
        &lt;blockquote>&#xD;
            &lt;p class=&quot;example&quot;>&#xD;
                RfT is the total number of Requirements for Test.&#xD;
            &lt;/p>&#xD;
        &lt;/blockquote>&#xD;
    &lt;/li>&#xD;
    &lt;li style=&quot;LIST-STYLE-TYPE: none&quot;>&#xD;
        Successful Test Coverage (executed) = T&lt;sup>&lt;sup>s&lt;/sup>&lt;/sup> / RfT&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;blockquote>&#xD;
    &lt;blockquote>&#xD;
        &lt;p class=&quot;example&quot;>&#xD;
            Where:&lt;br />&#xD;
            T&lt;sup>&lt;sup>s&lt;/sup>&lt;/sup> is the number of Tests executed, expressed as test procedures or test cases that&#xD;
            completed successfully, without defects.&#xD;
        &lt;/p>&#xD;
    &lt;/blockquote>&#xD;
    &lt;blockquote>&#xD;
        &lt;p class=&quot;example&quot;>&#xD;
            RfT is the total number of Requirements for Test.&#xD;
        &lt;/p>&#xD;
    &lt;/blockquote>&#xD;
&lt;/blockquote>&#xD;
&lt;div style=&quot;MARGIN-LEFT: 4em&quot;>&#xD;
    &lt;br />&#xD;
    &lt;br />&#xD;
&lt;/div>&#xD;
&lt;p>&#xD;
    Turning the above ratios into percentages allows for the following statement of requirements-based test coverage:&#xD;
&lt;/p>&#xD;
&lt;blockquote>&#xD;
    &lt;p>&#xD;
        x% of test cases (T&lt;sup>&lt;sup>(p,i,x,s)&lt;/sup>&lt;/sup> in the above equations) have been covered with a success rate of&#xD;
        y%&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;p>&#xD;
    This meaningful statement of test coverage can be matched against a defined success criteria. If the criteria have not&#xD;
    been met, then the statement provides a basis for predicting how much testing effort remains.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Code-based test coverage&quot; name=&quot;Code-based test coverage&quot;>Code-based Test Coverage&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Code-based test coverage measures how much code has been executed during the test, compared to how much code is left to&#xD;
    execute. Code coverage can be based on control flows (statement, branch, or paths) or data flows.&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        In control-flow coverage, the aim is to test lines of code, branch conditions, paths through the code, or other&#xD;
        elements of the software's flow of control.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        In data-flow coverage, the aim is to test that data states remain valid through the operation of the software; for&#xD;
        example, that a data element is defined before it's used.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Code-based test coverage is calculated by the following equation:&#xD;
&lt;/p>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        Test Coverage = I&lt;sup>&lt;sup>e&lt;/sup>&lt;/sup> / TIic&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        Where:&lt;br />&#xD;
        I&lt;sup>&lt;sup>e&lt;/sup>&lt;/sup> is the number of items executed, expressed as code statements, code branches, code paths,&#xD;
        data state decision points, or data element names.&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;blockquote>&#xD;
    &lt;p class=&quot;example&quot;>&#xD;
        TIic is the total number of items in the code.&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&lt;br />&#xD;
&lt;br />&#xD;
&lt;p>&#xD;
    Turning this ratio into a percentage allows the following statement of code-based test coverage:&#xD;
&lt;/p>&#xD;
&lt;blockquote>&#xD;
    &lt;p>&#xD;
        x% of test cases (I in the above equation) have been covered with a success rate of y%&#xD;
    &lt;/p>&#xD;
&lt;/blockquote>&#xD;
&lt;p>&#xD;
    This meaningful statement of test coverage can be matched against a defined success criteria. If the criteria have not&#xD;
    been met, then the statement provides a basis for predicting how much testing effort remains.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;b>&lt;a id=&quot;Quality&quot; name=&quot;Quality&quot;>Measuring Perceived Quality&lt;/a>&lt;/b>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Although evaluating test coverage provides a measure of the extent of completeness of the testing effort, evaluating&#xD;
    defects discovered during testing provides the best indication of the software quality as it has been experienced. This&#xD;
    perception of quality can be used to reason about the general quality of the software system as a whole. Perceived&#xD;
    Software Quality is a measure of how well the software meets the requirements levied on it, therefore, in this context,&#xD;
    defects are considered as a type of change request in which the target-of-test failed to meet the software&#xD;
    requirements.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Defect evaluation could be based on methods that range from simple defect counts to rigorous statistical modeling.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Rigorous evaluation uses assumptions about the arrival or discovery rates of defects during the testing process. A&#xD;
    common model assumes that the rate follows a Poisson distribution. The actual data about defect rates are then fit to&#xD;
    the model. The resulting evaluation estimates the current software reliability and predicts how the reliability will&#xD;
    grow if testing and defect removal continue. This evaluation is described as software-reliability growth modeling and&#xD;
    it's an area of active study. Due to the lack of tool support for this type of evaluation, you want to carefully&#xD;
    balance the cost of using this approach with the benefits gained.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;b>Defects analysis&lt;/b> involves analyzing the distribution of defects over the values of one or more of the attributes&#xD;
    associated with a defect. Defect analysis provides an indication of the reliability of the software.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In defect analysis, four main defect attributes are commonly analyzed:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;b>Status&lt;/b> - the current state of the defect (open, being fixed, closed, and so forth).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Priority&lt;/b> - the relative importance of this defect being addressed and resolved.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Severity&lt;/b> - the relative impact of this defect to the user, an organization, third parties, and so on.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Source&lt;/b> - where and what is the originating fault that results in this defect or what component will be fixed&#xD;
        to eliminate this defect.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Defect counts can be reported as a function of time, creating a Defect Trend diagram or report. They can also be&#xD;
    reported in a Defect Density Report as a function of one or more defect attributes, like severity or status. These&#xD;
    types of analysis provide a perspective on the trends or on the distribution of defects that reveal the software's&#xD;
    reliability.&lt;br />&#xD;
    &lt;br />&#xD;
    For example, it's expected that defect discovery rates will eventually diminish as the testing and fixing progresses. A&#xD;
    defect or poor quality threshold can be established at which point the software quality will be unacceptable. Defect&#xD;
    counts can also be reported based on the origin in the Implementation model, allowing for detection of &quot;weak modules&quot;,&#xD;
    &quot;hot spots&quot;, and parts of the software that keep being fixed again and again, which indicates more fundamental design&#xD;
    flaws.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Only confirmed defects are included in an analysis of this kind. Not all reported defects denote an actual flaw; some&#xD;
    might be enhancement requests outside of the project's scope, or may describe a defect that's already been reported.&#xD;
    However, it's valuable to look at and analyze why many defects, which are either duplicates or not confirmed defects,&#xD;
    are being reported.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Defect Reports&quot; name=&quot;Defect Reports&quot;>Defect Reports&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    The defect evaluation should be performed&amp;nbsp;based on multiple reporting categories, as follows:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Defect Distribution (Density) Reports allow defect counts to be shown as a function of one or two defect&#xD;
        attributes.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Defect Age Reports are a special type of defect distribution report. Defect age reports show how long a defect has&#xD;
        been in a particular state, such as Open. In any age category, defects can also be sorted by another attribute,&#xD;
        such as Owner.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Defect Trend Reports show defect counts, by status (new, open, or closed), as a function of time. The trend reports&#xD;
        can be cumulative or non-cumulative.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Many of these reports are valuable in assessing software quality. They are most useful when analyzed in conjunction&#xD;
    with Test results and progress reports that show the results of the tests conducted over a number of iterations and&#xD;
    test cycles for the application-under-test. The usual test criteria include a statement about the tolerable numbers of&#xD;
    open defects in particular categories, such as severity class, which is easily checked with an evaluation of defect&#xD;
    distribution. By sorting or grouping this distribution by test motivators, the evaluation can be focused on important&#xD;
    areas of concern.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Normally tool support is required to effectively produce reports of this kind.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;b>&lt;a id=&quot;Defect density reports:&quot; name=&quot;Defect density reports:&quot;>Defect Density Reports&lt;/a>&lt;/b>&#xD;
&lt;/h4>&#xD;
&lt;h5>&#xD;
    &lt;b>Defect status versus priority&lt;/b>&#xD;
&lt;/h5>&#xD;
&lt;p>&#xD;
    Give each defect a priority. It's usually practical and sufficient to have four levels of priority, such as:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Urgent priority (resolve immediately)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        High priority&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Normal priority&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Low priority&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    &lt;b>Note&lt;/b>: Criteria for a successful test could be expressed in terms of how the distribution of defects over these&#xD;
    priority levels should look. For example, successful test criteria might be &quot;no Priority 1 defects and fewer than five&#xD;
    Priority 2 defects are open&quot;. A defect distribution diagram, such as the following, should be generated.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;233&quot; alt=&quot;Defect Distribution Diagram&quot; src=&quot;./resources/keymeas1.gif&quot; width=&quot;378&quot; />&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br />&#xD;
&lt;p>&#xD;
    It's clear that the criteria has not been met. This diagram needs to include a filter to show only open defects, as&#xD;
    required by the test criteria.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    &lt;b>Defect status versus severity&lt;/b>&#xD;
&lt;/h5>&#xD;
&lt;p>&#xD;
    Defect Severity Reports show how many defects there are for each severity class; for example, fatal error, major&#xD;
    function not performed, minor annoyance.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    &lt;b>Defect status versus location in the Implementation model&lt;/b>&#xD;
&lt;/h5>&#xD;
&lt;p>&#xD;
    Defect Source Reports show distribution of defects on elements in the Implementation model.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;b>&lt;a id=&quot;Defect aging reports:&quot; name=&quot;Defect aging reports:&quot;>Defect Aging Reports&lt;/a>&lt;/b>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Defect Age Analysis provides good feedback on the effectiveness of the testing and the defect removal tasks. For&#xD;
    example, if the majority of older, unresolved defects are in a pending-validation state, it probably means that not&#xD;
    enough resources are applied to the retesting effort.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;b>&lt;a id=&quot;Defect trend reports:&quot; name=&quot;Defect trend reports:&quot;>Defect Trend Reports&lt;/a>&lt;/b>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Defect Trend Reports identify defect rates and provide a particularly good view of the state of the testing. Defect&#xD;
    trends follow a fairly predictable pattern in a testing cycle. Early in the cycle, the defect rates rise quickly, then&#xD;
    they reach a peak, and decrease at a slower rate over time.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br />&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;230&quot; alt=&quot;Defect Trend Reports Diagram&quot; src=&quot;./resources/keymeas2.gif&quot; width=&quot;373&quot; />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    To find problems, the project schedule can be reviewed in light of this trend. For example, if the defect rates are&#xD;
    still rising in the third week of a four-week test cycle, the project is clearly not on schedule.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    This simple trend analysis assumes that defects are being fixed promptly and that the fixes are being tested in&#xD;
    subsequent builds, so that the rate of closing defects should follow the same profile as the rate of finding defects.&#xD;
    When this does not happen, it indicates a problem with the defect-resolution process; the defect fixing resources or&#xD;
    the resources to retest and validate fixes could be inadequate.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;230&quot; alt=&quot;Trend Analysis Report Diagram&quot; src=&quot;./resources/keymeas3.gif&quot; width=&quot;469&quot; />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The trend reflected in this report shows that new defects are discovered and opened quickly at the beginning of the&#xD;
    project, and that they decrease over time. The trend for open defects is similar to that for new defects, but lags&#xD;
    slightly behind. The trend for closing defects increases over time as open defects are fixed and verified. These trends&#xD;
    depict a successful effort.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    If your trends deviate dramatically from these, they may indicate a problem and identify when additional resources need&#xD;
    to be applied to specific areas of development or testing.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    When combined with the measures of test coverage, the defect analysis provides a very good assessment on which to base&#xD;
    the test completion criteria.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Performance&quot; name=&quot;Performance&quot;>Performance Measures&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Several measures are used for assessing the performance behaviors of the target-of-test and for focusing on capturing&#xD;
    data related to behaviors such as response time, timing profiles, execution flow, operational reliability, and limits.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The primary performance measures include:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;b>Dynamic Monitoring&lt;/b> - real-time capture and display of the status and state of each test script being&#xD;
        executed during the test execution.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Response Time and Throughput Reports&lt;/b> - measurement of the response times and throughput of the&#xD;
        target-of-test for specified actors and use cases.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Percentile Reports&lt;/b> - percentile measurement and calculation of the data collected values.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Comparison Reports&lt;/b> - differences or trends between two (or more) sets of data representing different test&#xD;
        executions.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Trace Reports&lt;/b> - details of the messages and conversations between the actor (test script) and the&#xD;
        target-of-test.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Dynamic Monitoring&quot; name=&quot;Dynamic Monitoring&quot;>Dynamic Monitoring&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Dynamic monitoring provides real-time display and reporting during test execution, typically in the form of a histogram&#xD;
    or a graph. The report monitors or assesses performance test execution by displaying the current state, status, and&#xD;
    progress of the test scripts.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;333&quot; alt=&quot;Dynamic Monitoring Displayed as a Histogram&quot; src=&quot;./resources/keymeas4.gif&quot; width=&quot;501&quot; />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    For example, in the preceding histogram, there are 80 test scripts executing the same use case. In this graph, 14 test&#xD;
    scripts are in the Idle state, 12 in the Query, 34 in SQL Execution, 4 in SQL Connect, and 16 in the Other state. As&#xD;
    the test progresses, you would expect to see the number of scripts in each state change. The displayed output would be&#xD;
    typical of a test execution that is executing normally and is in the middle of its execution. However, if test scripts&#xD;
    remain in one state or do not show changes during test execution, this could indicate a problem with the test&#xD;
    execution, or the need to implement or evaluate other performance measures.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Response time and throughput reports&quot; name=&quot;Response time and throughput reports&quot;>Response Time and Throughput&#xD;
    Reports&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Response Time and Throughput Reports, as their name implies, measure and calculate the performance behaviors related to&#xD;
    time and throughput (number of transactions processed). Typically, these reports are displayed as a graph with response&#xD;
    time (or number of transactions) on the &quot;y&quot; axis and events on the &quot;x&quot; axis.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;333&quot; alt=&quot;Sample Throughput and Analysis Report Diagram&quot; src=&quot;./resources/keymeas5.gif&quot; width=&quot;499&quot; />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    It's often valuable to calculate and display statistical information, such as the mean and standard deviation of the&#xD;
    data values in addition to showing the actual performance behaviors.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Percentile Reports&quot; name=&quot;Percentile Reports&quot;>Percentile Reports&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Percentile Reports provide another statistical calculation of performance by displaying population percentile values&#xD;
    for data types collected.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;333&quot; alt=&quot;Sample Percentile Report Diagram&quot; src=&quot;./resources/keymeas6.gif&quot; width=&quot;499&quot; />&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Comparison Reports&quot; name=&quot;Comparison Reports&quot;>Comparison Reports&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    It's important to compare the results of one performance test execution with that of another, so you can evaluate the&#xD;
    impact of changes made between test executions on the performance behaviors. Use Comparison Reports to display the&#xD;
    difference between two sets of data (each representing different test executions) or trends between many executions of&#xD;
    test.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Trace and Profile Reports&quot; name=&quot;Trace and Profile Reports&quot;>Trace and Profile Reports&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    When performance behaviors are unacceptable or when performance monitoring indicates possible bottlenecks (such as when&#xD;
    test scripts remain in a given state for exceedingly long periods), trace reporting could be the most valuable report.&#xD;
    Trace and Profile Reports display lower-level information. This information includes the messages between the actor and&#xD;
    the target-of-test, execution flow, data access, and the function and system calls.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br /></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
