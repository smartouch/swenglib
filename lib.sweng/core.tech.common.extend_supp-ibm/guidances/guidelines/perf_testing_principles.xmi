<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-p7JQ58uCGazfjIHV_Y0rwg" name="perf_testing_principles,_0EWIAHODEdyur-UVVhx-aQ" guid="-p7JQ58uCGazfjIHV_Y0rwg" version="7.1.0">
  <mainDescription>&lt;h3>&#xD;
    Introduction&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    This general guideline discusses some key principles for performance testing. It covers the following major tasks:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Discover%20Your%20Customer's%20Goals&quot;>Discover Your Customer's Goals&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Create%20a%20Detailed%20Workload%20Model&quot;>Create a Detailed Workload Model&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Simplify%20the%20Customer's%20Workload&quot;>Simplify the Customer's Workload&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Gain%20Agreement%20on%20the%20Workload%20Model&quot;>Gain Agreement on the Workload Model&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Specify%20What%20Measurements%20are%20Significant&quot;>Specify What Measurements are Significant&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Maintain%20Control%20of%20the%20Test%20Environment&quot;>Maintain Control of the Test Environment&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Report%20the%20Test%20Results%20in%20the%20Context%20of%20the%20Workload%20Specification%20and%20Lab%20Environment&quot;>Report&#xD;
        the Test Results in the Context of the Workload Specification and Lab Environment&lt;/a>&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Formally%20Document%20Results%20and%20Recommendations&quot;>Formally Document Results and Recommendations&lt;/a>&#xD;
    &lt;/li>&#xD;
&lt;/ul>&lt;br />&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Discover Your Customer's Goals&quot; name=&quot;Discover Your Customer's Goals&quot;>Discover Your Customer's Goals&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    There are many reasons to do a performance test. Some of these include high availability (long running stability under&#xD;
    load), system capacity (how many users or transactions can I do?), or service level agreement (do the key transactions&#xD;
    return in under X seconds?). Find out from your customer exactly what questions need to be answered so you know how to&#xD;
    design your tests and lab experiments. By formulating and documenting testing goals, you will have a good basis to&#xD;
    begin building your workload model.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Create a Detailed Workload Model&quot; name=&quot;Create a Detailed Workload Model&quot;>Create a Detailed Workload Model&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The key to a successful and orderly (well-run) performance test is developing a detailed workload model. You will need&#xD;
    access to either a statistical breakdown of an existing production workload, or the ability to synthesize the workload&#xD;
    by gathering the equivalent data with analysts from the customer's business operations department.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    With the help of the analysts, you will need to answer questions like the following:&#xD;
&lt;/p>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        What are the most frequently executed 20% of the transactions, which probably generate 80% of the total system&#xD;
        load?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        What are the expected transaction rates for each of these transaction types?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Which data inputs are the independent variables that spread the data accesses across any back-end databases&#xD;
        involved?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Are there classes of users on the system who do very different sets of transactions that each need to be modeled&#xD;
        separately?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        How many of each user class are simultaneously active during the busiest hour of the day?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Are there any variable data design or uniqueness constraints on the data inputs?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Are there any background batch jobs or reports that cause a significant portion of the system load (even though&#xD;
        their frequency is not in the top 20% of the operations)?&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;p>&#xD;
    Based on the answers to these and other similar questions, you need to construct a user scenario for each of the&#xD;
    transactions or other operations contained in the workload model. If types of users have been identified with relative&#xD;
    or absolute numbers of users in each, then these need to be specified in the workload model. If user types are not&#xD;
    used, then cumulative transaction rates need to be translated into numbers of simultaneous user scenarios, which are&#xD;
    then used to accomplish the total workload transaction rates.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    You need to be specific about average think times between user interactions and average transaction rates, and then&#xD;
    identify data inputs for variation. You also need to identify key transactions for measurement and tuning activities,&#xD;
    as well as expected response time values for each.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The modeled &lt;i>Busy Hour&lt;/i> should be explicitly called out and associated with the workload characteristics.&#xD;
    Constraints on the size and composition of the test environment data state at the beginning of each test run need to be&#xD;
    explicitly called out in the workload specification.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Simplify the Customer's Workload&quot; name=&quot;Simplify the Customer's Workload&quot;>Simplify the Customer's Workload&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Most of the time, the customer will want much too much of the real workload modeled in the performance test. Work with&#xD;
    the customer to agree on as many simplifications of the workload model as can be done while maintaining the overall&#xD;
    usefulness of the prediction capability of the model. As long as the workload model accurately predicts the Busy Hour&#xD;
    performance of the system and adequately stresses all of the subsystems with approximate work done under production&#xD;
    conditions, the model is useful for predicting the production system's behavior.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    This approximate behavior of the workload model is the essence of doing valuable performance testing. For example, if&#xD;
    there are three variations on how to enter a new customer record into a customer database, pick the most frequent or&#xD;
    the one that is closest (by some estimation) to the average user scenario. Then combine the transaction rates for all&#xD;
    three variations and use the average user scenario in the workload model.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Ideally the notion of simplifying the workload yields a much quicker implementation of the workload model, and makes it&#xD;
    both more flexible and adaptable to use in the future as the customer's workload changes over time. Adjustments can be&#xD;
    made to transaction rates, database sizes, data input variations, and total volume of work without having to implement&#xD;
    new or different user scenarios. This concept makes a workload model reusable and useful to predict the answers to many&#xD;
    &quot;what-if&quot; scenarios posed by the customer, both now and in in the future.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Gain Agreement on the Workload Model&quot; name=&quot;Gain Agreement on the Workload Model&quot;>Gain Agreement on the Workload&#xD;
    Model&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Because of the abstraction contained in the workload model, you need to sit down with the customer and explain the&#xD;
    details of the workload model, including both what it does model and what it doesn't. All of this needs to be done with&#xD;
    the customer &lt;i>before&lt;/i> implementation of the model begins. Doing this -- gaining acceptance of the workload model&#xD;
    content ahead of time, in an atmosphere of collaboration -- results in later meetings with the customer being centered&#xD;
    on the test results and not whether the model was correct.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    These later meetings tend to be filled with emotion, because the customer feels pressure since major business decisions&#xD;
    often ride on the outcome of the performance test measurements. If the customer is not familiar with the workload model&#xD;
    employed in the test, the focus shifts to understanding the model and figuring out if it is a true predictor of&#xD;
    production system performance. This is counterproductive after the workload model is implemented and being used, as the&#xD;
    focus needs to be on system tuning and potentially re-hosting or re-implementing some of the system before placing it&#xD;
    into production. These are costly and typically time-critical decisions that should be the focus of the customer&#xD;
    meetings once measurement data is available.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Specify What Measurements are Significant&quot; name=&quot;Specify What Measurements are Significant&quot;>Specify What&#xD;
    Measurements are Significant&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Often there are a small number of the transaction types or critical business scenarios that contain measurement points&#xD;
    that signal whether or not the system is behaving within acceptable parameters. Usually different operations result in&#xD;
    different enough response time values that they should not be statistically grouped together since, when taken as a&#xD;
    single sample set, they really do not predict the behavior of any operation.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    You should specify in the workload model which of these measurements are the important ones to monitor, and also have a&#xD;
    time threshold of acceptability values for each of these measurement points. Much of the system tuning efforts should&#xD;
    be focused on achieving these values through system parameter adjustments, and in some cases coding adjustments within&#xD;
    the application. Since these key measurements are the one most heavily scrutinized, they should be chosen carefully in&#xD;
    conjunction with the customer and their business analysts as part of the workload model development.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Usually some peripheral measurements come up with unacceptable values and result in changes to the system. However, the&#xD;
    key measurements point to whether or not the system is ready for production deployment, and are central to that&#xD;
    decision.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Maintain Control of the Test Environment&quot; name=&quot;Maintain Control of the Test Environment&quot;>Maintain Control of&#xD;
    the Test Environment&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The performance testing laboratory environment has a large number of components, the states of which must be tightly&#xD;
    controlled so that changes made between test runs are well understood. This promotes predictability and&#xD;
    reproduceability of performance test results.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Normally you should keep two change logs, one for the test driver complex and one for the system under test complex.&#xD;
    The change log for the performance test driver complex is the record of changes to the hardware and software networking&#xD;
    fabric specifically not part of the system to be deployed. The change log for the system under test is the record of&#xD;
    all changes to the hardware, software, and networking that need to be reflected in the production system once it is&#xD;
    deployed. System tuning changes are very important to keep in the log and associate with specific test results. This&#xD;
    ensures that once an experiment has been tried and results gathered, the experiment will not have to be repeated just&#xD;
    to reconstruct the results because there was some uncertainty about the system state when certain results were&#xD;
    recorded.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    If results impacting changes are being made to the driver complex, such as a change to the user think times or some&#xD;
    transaction rates, no meaningful tuning comparisons can be made to previous test runs. Since the workload changed, the&#xD;
    results must be re-baselined so that new tuning experiments can be compared before and after the change was attempted.&#xD;
    Since these large performance labs can consist of dozens of systems and networking components, an accurate account of&#xD;
    all changes is crucial to make progress towards a tuned and fully characterized system.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Report the Test Results in the Context of the Workload Specification and Lab Environment&quot; name=&quot;Report the Test Results in the Context of the Workload Specification and Lab Environment&quot;>Report the Test Results&#xD;
    in the Context of the Workload Specification and Lab Environment&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Often there are many dynamic shifts in tactics between consecutive performance test runs, based on the measured results&#xD;
    from the previous run. Having a general test strategy and daily objectives during the process are important so that you&#xD;
    can make progress towards achieving the basic objectives of your test. The alternative to a constantly changing test&#xD;
    plan is to be pragmatic about the necessary adjustments during the test interval, and respond by keeping a separate&#xD;
    test log of experiments performed. Each test run should be logged, including excerpts from both change logs, as well as&#xD;
    what load level of the workload specification was being run during the experiment. The results associated with that run&#xD;
    are then logged with the environment data.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The test plan becomes a high-level test strategy with mid point objectives, such as:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Have the application servers tuned by Tuesday&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Perform database tuning on Wednesday and Thursday&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Reserve Friday for any other subsystem tuning&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    The final system measurements could then be made the following week between Monday and Wednesday.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Formally Document Results and Recommendations&quot; name=&quot;Formally Document Results and Recommendations&quot;>Formally&#xD;
    Document Results and Recommendations&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Once all of the experiments have been run and the detailed results documented, the performance test analyst goes back&#xD;
    and summarizes for the customer the steps taken along the way -- during the entire testing interval -- and the&#xD;
    incremental results achieved. You should then give the customer a bottom line conclusion about the current state of the&#xD;
    system, as well as what steps should now be taken to move the system from its current state to where it fully meets all&#xD;
    of the objectives for the performance test. You should also document any recommended procedures or test environment&#xD;
    changes to make the next performance test run more smoothly. This final report should be very high quality, and have a&#xD;
    complete appendix or reference to the detailed test results. It should also reference the workload specification where&#xD;
    the testing goals were documented in detail.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Presenting this final report to the customer is the culmination of the performance test, and should provide a complete&#xD;
    communication of the summarized findings of the test.&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
