<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-Dj2s6K7NJdCWJPV7COU0Ug" name="new_guideline,_XyqTAMUwEdyJCceLNDhCjA" guid="-Dj2s6K7NJdCWJPV7COU0Ug" changeDate="2008-01-17T11:20:01.812-0800" version="7.2.0">
  <mainDescription>&lt;h3>&#xD;
    Overview&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Software quality is assessed along different dimensions, including reliability, function, and performance.&amp;nbsp;You&#xD;
    have&amp;nbsp;to identify and define the different variables that affect or influence an application or system's&#xD;
    performance and the measures required to assess performance. The workload profiles represent candidates for conditions&#xD;
    to be simulated against the Target Test Items under one or more Test Environment Configurations. Different roles in the&#xD;
    organization need this kind of information for different purposes:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        the &lt;b>test analyst:&lt;/b> to identify test ideas and define test cases for different tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>test designer:&lt;/b> to define an appropriate test approach and identify testability needs for the different&#xD;
        tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>tester:&lt;/b> to better understand the goals of the test to implement, execute and analyze its execution&#xD;
        properly&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>user representative:&lt;/b> to assess the appropriateness of the workload, and the tests required to&#xD;
        effectively assess the systems behavior against that workload&amp;nbsp;profiles&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    The captured information focuses on characteristics and attributes in the following primary areas:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Use-Case Scenarios to be executed and evaluated during the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Actors to be simulated / emulated during the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Workload profile - representing the number and type of simultaneous actor instances, use-case scenarios executed by&#xD;
        those actor instances, and on-line responses or throughput associated with each use-case scenario.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test Environment Configuration (actual, simulated or emulated) to be used in executing and evaluating the tests&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Tests should be considered to measure and evaluate the characteristics and behaviors of the target-of-test when&#xD;
    functioning under different workloads. Successfully designing, implementing, and executing these tests requires&#xD;
    identifying both realistic and exceptional data for these workload profiles.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Use Cases and Use Case Attributes&quot; name=&quot;Use Cases and Use Case Attributes&quot;>Use Cases and Use Case&#xD;
    Attributes&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Two aspects of use cases are considered for selection of scenarios for this type of testing:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Critical Use Cases&amp;nbsp;contain the key use-case scenarios to be measured and evaluated in the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Significant Use&amp;nbsp;Cases&amp;nbsp;contain use-case scenarios that may impact the behavior of the critical use-case&#xD;
        scenarios&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Critical Use Cases&quot; name=&quot;Critical Use Cases&quot;>Critical Use Cases&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Not all use-case scenarios being implemented in the target-of-test may be needed for these tests. Critical use cases&#xD;
    contain those use-case scenarios that will be the focus of the test - that is their behaviors will be measured and&#xD;
    evaluated.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    To identify the critical use cases, identify those use-case scenarios that meet one or more of the following criteria:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        require measurement and assessment based on workload profile&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        are executed frequently by one or more end-users (actor instances)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        that represent a high percentage of system use&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        that consume significant system resources&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    List the critical use-case scanners for inclusion in the test. As theses are being identified, the use case flow of&#xD;
    events should be reviewed. Begin to identify the specific sequence of events between the actor (type) and system when&#xD;
    the use-case scenario is executed.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Additionally, identify (or verify) the following information:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Preconditions for the use cases, such as the state of the data (what data should / should not exist) and the state&#xD;
        of the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Data that may be constant (the same) or must differ from one use-case scenario to the next&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Relationship between the use case and other use cases, such as the sequence in which the use cases must be&#xD;
        performed.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The frequency of execution of the use-case scenario, including characteristics such as the number of simultaneous&#xD;
        instances of the use case and the percent of the total load each scenario places on the system.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Significant Use Cases&quot; name=&quot;Significant Use Cases&quot;>Significant Use Cases&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Unlike critical use-case scenarios, which are the primary focus of the test, significant use-case scenarios are those&#xD;
    that may impact the performance behaviors of critical use-case scenarios. Significant use-case scenarios include those&#xD;
    that meet one or more of the following criteria:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        they must be executed before or after executing a critical use case (a dependent precondition or postcondition)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they are executed frequently by one or more actor instances&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they represent a high percentage of system use&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they require significant system resources&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they will be executed routinely on the deployed system while critical use-case scenarios are executed, such as&#xD;
        e-mail or background printing&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    As the significant use-case scenarios are being identified and listed, review the use case flow of events and&#xD;
    additional information as done above for the critical use-case scenarios.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Actors and Actor Attributes&quot; name=&quot;Actors and Actor Attributes&quot;>Actors and Actor Attributes&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Successful performance tests requires identifying not just the actors executing the critical and significant use-case&#xD;
    scenarios, but must also simulate / emulate actor behavior. That is, one instance of an actor may interact with the&#xD;
    target-of-test differently (take longer to respond to prompts, enter different data values, etc.) while executing the&#xD;
    same use-case scenario as another instance of that actor. Consider the simple use cases below:&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;226&quot; alt=&quot;Diagram described in caption.&quot; src=&quot;./resources/tstcs002.gif&quot; width=&quot;354&quot; />&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot;>&#xD;
    Actors and use cases in an ATM machine.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The first instance of the &quot;Customer&quot; actor executing a use-case scenario might be an experienced ATM user, while&#xD;
    another instance of the &quot;Customer&quot; actor may be inexperienced at ATM use. The experienced Customer quickly navigates&#xD;
    through the ATM user-interface and spends little time reading each prompt, instead, pressing the buttons by rote. The&#xD;
    inexperienced Customer however, reads each prompt and takes extra time to interpret the information before responding.&#xD;
    Realistic workload profiles reflect this difference to ensure accurate assessment of the behaviors of the&#xD;
    target-of-test.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Begin by identifying the actors for each use-case scenario identified above. Then identify the different actor profiles&#xD;
    that may execute each use-case scenario. In the ATM example above, we may have the following actor stereotypes:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Experienced ATM user&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Inexperienced ATM user&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        ATM user's account is &quot;inside&quot; the ATM's bank network (user's account is with bank owning ATM)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        ATM user's account is outside the ATM's bank network (competing bank)&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    For each actor profile, identify the different attributes and their values such as:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Think time - the period of time it takes for an actor to respond to a target-of-test's individual prompts&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Typing rate - the rate at which the actor interacts with the interface&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Request Pace - the rate at which the actor makes requests of the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Repeat factor - the number of times a use case or request is repeated in sequence&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Interaction method - the method of interaction used by the actor, such as using the keyboard to enter in values,&#xD;
        tabbing to a field, using accelerator keys, etc., or using the mouse to &quot;point and click&quot;, &quot;cut and paste&quot;, etc.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Additionally, for each actor profile identify their workload profile, specifying all the use-case scenarios they&#xD;
    execute, and the percentage of time or proportion of effort spent by the actor executing these scenarios. Identifying&#xD;
    this information is used in identifying and creating a realistic load (see Load and Load Attributes below).&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;System Attributes and Variables&quot; name=&quot;System Attributes and Variables&quot;>System Attributes and Variables&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The specific attributes and variables of the Test Environment Configuration that uniquely identify the environment must&#xD;
    also be identified, as these attributes also impact the measurement and evaluation of behavior. These attributes&#xD;
    include:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        The physical hardware (CPU speed, memory, disk caching, etc.)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The deployment architecture (number of servers, distribution of processing, etc.)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The network attributes&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Other software (and use cases) that may be installed and executed simultaneously to the target-of-test&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Identify and list the system attributes and variables that are to be considered for inclusion in the tests. This&#xD;
    information may be obtained from several sources, including: Vision or Software Architecture documents, or Stakeholder&#xD;
    requests.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Workload Profiles&quot; name=&quot;Workload Profiles&quot;>Workload Profiles&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    As stated previously, workload is an important factor that impacts the behavior of a target-of-test. Accurately&#xD;
    identifying the workload profile that will be used to evaluate the targets behavior is critical. Typically, test that&#xD;
    involve workload are executed several times using different workload profiles, each representing a variation of the&#xD;
    attributes described below:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        The number of simultaneous actor instances interacting with the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The profile of the actors interacting with the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The use-case scenarios executed by each actor instance&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The frequency of each critical use-case scenarios executed and how often it is repeated&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    For each workload profile used to evaluate the performance of the target-of-test, identify the values for each of the&#xD;
    above variables. The values used for each variable in the different loads may be derived by observing or interviewing&#xD;
    actors or, from the Business Use-Case Model&amp;nbsp; if one is available. It is common for one or more of the following&#xD;
    workload profiles to be defined:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Optimal - a workload profile that reflects the best possible deployment conditions, such as a minimal number of&#xD;
        actor instances interacting with the system, executing only the critical use-case scenarios, with minimal&#xD;
        additional software and workload executing during the test.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Average (AKA Normal) - a workload profile that reflects the anticipated or actual average usage conditions.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Instantaneous Peak - a workload profile that reflects anticipated or actual instantaneous heavy usage conditions,&#xD;
        that occur for short periods during normal operation.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Peak - a workload profile that reflects anticipated or actual heavy usage conditions, such as a maximum number of&#xD;
        actor instances, executing high volumes of use-case scenarios, with much additional software and workload executing&#xD;
        during the test.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    When workload testing includes Stress Testing, several additional loads should be identified, each targeting specific&#xD;
    aspects of the system in abnormal or unexpected states beyond the expected normal capacity of the deployed system.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Performance Measurements and Criteria&quot; name=&quot;Performance Measurements and Criteria&quot;>Performance Measurements and&#xD;
    Criteria&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Successful workload testing can only be achieved if the tests are measured and the workload behaviors evaluated. In&#xD;
    identifying workload measurements and criteria, the following factors should be considered:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        What measurements are to be made?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Where / what are the critical measurement points in the target-of-test / use-case execution.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        What are the criteria to be used for determining acceptable performance behavior?&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    Performance Measurements&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    There are many different measurements that can be made during test execution. Identify the significant measurements to&#xD;
    be made and justify why they are the most significant measurements.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Listed below are the more common performance behaviors monitored or captured:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Test script state or status - a graphical depiction of the current state, status, or progress of the test execution&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Response time / Throughput - measurement (or calculation) of response times or throughput (usually stated as&#xD;
        transactions per second).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;font color=&quot;#000000&quot;>Traces - capturing the messages / conversations between the actor (test script) and the&#xD;
        target-of-test, or the dataflow and / or process flow during execution.&lt;/font>&amp;nbsp;&amp;nbsp;&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    Critical Performance Measurement Points&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    In the Use Cases and Use Case Attributes section above, it was noted that not all use cases and their scenarios are&#xD;
    executed for performance testing. Similarly, not all performance measures are made for each executed use-case scenario.&#xD;
    Typically only specific use-case scenarios are targeted for measurement, or there may be a specific sequence of events&#xD;
    within a specific use-case scenario that will be measured to assess the performance behavior. Care should be taken to&#xD;
    select the most significant starting and ending &quot;points&quot; for the measuring the performance behaviors. The most&#xD;
    significant ones are typically those the most visible sequences of events or those that we can affect directly through&#xD;
    changes to the software or hardware.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    For example, in the ATM - Cash Withdraw use case identified above, we may measure the performance characteristics of&#xD;
    the entire use-case instance, from the point where the Actor initiates the withdrawal, to the point in which the use&#xD;
    case is terminated - that is, the Actor receives their bank card and the ATM is now ready to accept another card, as&#xD;
    shown by the black &quot;Total Elapsed Time&quot; line in the diagram below:&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;514&quot; alt=&quot;Diagram is described in the content.&quot; src=&quot;./resources/md_wlmd1.gif&quot; width=&quot;384&quot; />&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Notice, however, there are many sequences of events that contribute to the total elapsed time, some that we may have&#xD;
    control over (such as read card information, verify card type, initiate communication with bank system, etc., items B,&#xD;
    D, and E above), but other sequences, we have not control over (such as the actor entering their PIN or reading the&#xD;
    prompts before entering their withdrawal amount, items A, C, and F). In the above example, in addition to measuring the&#xD;
    total elapsed time, we would measure the response times for sequences B, D, and E, since these events are the most&#xD;
    visible response times to the actor (and we may affect them via the software / hardware for deployment).&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    Performance Measurement Criteria&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Once the critical performance measures and measurement points have been identified, review the performance criteria.&#xD;
    If necessary revise the criteria.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Here are some criteria that are often used for performance measurement:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        response time (AKA on-line response)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        throughput rate&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        response percentiles&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    On-line response time, measured in seconds, or transaction throughput rate, measured by the number of transactions (or&#xD;
    messages) processed is the main criteria.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    For example, using the Cash Withdraw use case, the criteria is stated as &quot;events B, D, and E (see diagram above) must&#xD;
    each occur in under 3 seconds (for a combined total of 9 seconds)&quot;. If during testing, we note that that any one of the&#xD;
    events identified as B, D, or E takes longer than the stated 3 second criteria, we would note a failure.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Percentile measurements are combined with the response times and / or throughput rates and are used to &quot;statistically&#xD;
    ignore&quot; measurements that are outside of the stated criteria. For example, the performance criteria for the use case&#xD;
    was now states &quot;for the 90th percentile, events B, D, and E must each occur in under 3 seconds ...&quot;. During test&#xD;
    execution, if we measure 90 percent of all performance measurements occur within the stated criteria, no failures are&#xD;
    noted.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br /></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
