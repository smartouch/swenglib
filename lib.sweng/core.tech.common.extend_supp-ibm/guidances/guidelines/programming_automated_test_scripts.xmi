<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-7oWxw_lq2qYG16KsZdm1Rw" name="programming_automated_test_scripts,_rwftUHHUEdyzS55ez-koKA" guid="-7oWxw_lq2qYG16KsZdm1Rw" changeDate="2009-02-18T17:12:51.890-0800" version="7.2.0">
  <mainDescription>&lt;h3>&#xD;
    &lt;a id=&quot;StructureOfTestScripts&quot; name=&quot;StructureOfTestScripts&quot;>Structure of Test Scripts&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    To increase the maintainability and reusability of your Test Scripts, they should have been structured before they are&#xD;
    implemented. You will probably find that there are actions that will appear in several Test Scripts. A goal should be&#xD;
    to identify these actions so that you can reuse their implementation.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    For example, you may have Test Scripts that are combinations of different actions you can perform to a record. These&#xD;
    Test Scripts may be combinations of the addition, modification, and the deletion of a record:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Add, Modify, Delete (the obvious one)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Add, Delete, Modify&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Add, Delete, Add, Delete, ...&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Add, Add, Add, ...&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    If you identify and implement these actions as separate Test Scripts and reuse them in other Test Scripts you will&#xD;
    achieve a higher level of reuse.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Another goal would be to structure your Test Scripts in such a way that a change in the target software causes a&#xD;
    localized and controllable change in your Test Scripts. This will make your Test Scripts more resilient to changes in&#xD;
    the target software. For example, say the log-in portion of the software has changed. For all test cases that traverses&#xD;
    the log-in portion, only the Test Script pertaining to log-in will have to change.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;RecordingTechnique&quot; name=&quot;RecordingTechnique&quot;>Recording Technique&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    To achieve higher maintainability of your test scripts, you should record them in a way that is least vulnerable to&#xD;
    changes in the target-of-test. For example, for a test script that fills in dialog box fields, there are choices for&#xD;
    how to proceed from one field to the next:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Use the TAB key&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Use the mouse&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Use the keyboard accelerator keys&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Of these choices, some are more vulnerable to design changes than others. If a new field is inserted on the screen the&#xD;
    TAB key approach will not be reliable. If accelerator keys are reassigned, they will not provide a good recording. If&#xD;
    the method that the mouse uses to identify a field is subject to change, that may not be a reliable method either.&#xD;
    However, some test automation tools have test script recorders that can be instructed to identify the field by a more&#xD;
    reliable method, such as its Object Name assigned by the development tool (PowerBuilder, SQLWindows, or Visual Basic).&#xD;
    In this way, a recorded test script is not effected by minor changes to the user interface (for example, layout&#xD;
    changes, field label changes, formatting changes, etc.)&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Data-DrivenTesting&quot; name=&quot;Data-DrivenTesting&quot;>Data-Driven Testing&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Many Test Scripts involve entering several sets of field data in a given data entry screen to check field validation&#xD;
    functions, error handling, and so on. The procedural steps are the same; only the data is different. Rather than&#xD;
    recording a Test Script for every set of input data, a single recording should be made and then modified to handle&#xD;
    multiple data sets. For example, all the data sets that produce the same error because of invalid data can share the&#xD;
    same recorded Test Script. The Test Script is modified to address the data as variable information, to read the data&#xD;
    sets from a file or other external source, and to loop through all of the relevant data sets.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    If Test Scripts or test code have been developed to loop through sets of input and output data the data sets must be&#xD;
    established. The usual format to use for these data sets is records of comma-separated fields in a text file. This&#xD;
    format is easy to read from Test Scripts and test code, and is easy to create and maintain.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Most database and spreadsheet packages can produce comma-separated textual output. Using these packages to organize or&#xD;
    capture data sets has two important benefits. First, they provide a more structured environment for entering and&#xD;
    editing the data than simply using a text editor or word processor. Second, most have the ability to query existing&#xD;
    databases and capture the returned data, allowing an easy way to extract data sets from existing sources.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;ErrorHandling&quot; name=&quot;ErrorHandling&quot;>Error Handling&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The recorded Test Script is sequential in its execution. There are no branch points. Robust error handling in the Test&#xD;
    Scripts requires additional logic to respond to error conditions. Decision logic that can be employed when errors occur&#xD;
    includes:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Branching to a different Test Script.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Calling a script that attempts to clean up the error condition.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Exiting the script and starting the next one.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Exiting the script and the software, restarting, and resuming testing at the next Test Script after the one that&#xD;
        failed.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Each error-handling technique requires program logic added to the Test Script. As much as possible, this logic should&#xD;
    be confined to the high-level Test Scripts that control the sequencing of lower-level Test Scripts. This allows the&#xD;
    lower-level Test Scripts to be created completely from recording.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;TestScriptSynchronizationAndScheduling&quot; name=&quot;TestScriptSynchronizationAndScheduling&quot;>Test Script&#xD;
    Synchronization and Scheduling&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    When doing stress testing, it is often desirable to synchronize Test Scripts so that they start at predefined times.&#xD;
    Test Scripts can be modified to start at a particular time by comparing the desired start time with the system time. In&#xD;
    networked systems each test station will share, via the network, the same clock. In the following example (from a&#xD;
    script written in BASIC) statements have been inserted at the start of a script to suspend the execution of the script&#xD;
    until the required time is reached.&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;example&quot;>&#xD;
    &lt;code>InputFile$ = &quot;TIME.DAT&quot;&lt;br />&#xD;
    Open InputFile$ For Input As 1&lt;br />&#xD;
    Input #1, StartTime$&lt;br />&#xD;
    Close #1&lt;br />&#xD;
    Do While TimeValue(StartTime$) &amp;gt; Time&lt;br />&#xD;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;DoEvents&lt;br />&#xD;
    Loop&lt;/code>&lt;br />&#xD;
    [Start script]&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In this example, the required start time is stored in a file. This allows the start time to be changed without changing&#xD;
    the Test Script. The time is read and stored in a variable called &lt;code>StartTime$&lt;/code>. The &lt;code>Do While&lt;/code>&#xD;
    loop continues until the starting time is reached. The &lt;code>DoEvents&lt;/code> statement is important: it allows&#xD;
    background tasks to execute while the Test Script is suspended and waiting to start. Without the &lt;code>DoEvents&lt;/code>&#xD;
    statement, the system would be unresponsive until the start time had been reached.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;TestingAndDebuggingTestScripts&quot; name=&quot;TestingAndDebuggingTestScripts&quot;>Testing and Debugging Test Scripts&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    When the newly recorded Test Scripts are executed on the same software on which they were recorded, there should be no&#xD;
    errors. The environment and the software are identical to when it was recorded. There may be instances where the Test&#xD;
    Script does not run successfully. Testing the Test Scripts uncovers these cases, and allows the scripts to be corrected&#xD;
    before being used in a real test. Two typical kinds of problems are discussed here:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Ambiguity in the methods used for selecting items in a user interface can make Test Scripts operate differently&#xD;
        upon playback. For example, two items recognized by their text (or caption) may have identical text. There will be&#xD;
        ambiguity when the script is executed.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test run/session specific data is recorded (for example,&amp;nbsp;a pointer, date/timestamp or some other system&#xD;
        generated data value), but is different upon playback.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Timing differences in recording and playback can lead to problems. Recording a Test Script is inherently a slower&#xD;
    process than executing it. Sometimes this time difference results in the Test Script running ahead of the software. In&#xD;
    these cases, Wait States can be inserted to throttle the Test Script to the speed of the software.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br /></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
