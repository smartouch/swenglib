<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-_QkBIGZy-fgSxgVpCN90XQ" name="new_guideline,_3ain8H3FEd2eJPu0NyCdUg" guid="-_QkBIGZy-fgSxgVpCN90XQ" changeDate="2009-02-22T20:58:00.312-0800" version="7.5.0">
  <mainDescription>&lt;h3>&#xD;
    &lt;a id=&quot;Realizing Concurrency: Mechanisms&quot; name=&quot;Realizing Concurrency: Mechanisms&quot;>Realizing Concurrency:&#xD;
    Mechanisms&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Managing threads of control&quot; name=&quot;Managing threads of control&quot;>Managing Threads of Control&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    To support concurrency, a system must provide for multiple threads of control. The abstraction of a thread of control&#xD;
    can be implemented in a number of ways by hardware and software. The most common mechanisms are variations of one of&#xD;
    the following &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../../core.tech.common.extend-ibm/guidances/supportingmaterials/tech_references_17D34A60.html#DEI84&quot; guid=&quot;_OKj94D8iEd2AxfTL63EayA&quot;>[DEI84]&lt;/a>, &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../../core.tech.common.extend-ibm/guidances/supportingmaterials/tech_references_17D34A60.html#TAN86&quot; guid=&quot;_OKj94D8iEd2AxfTL63EayA&quot;>[TAN86]&lt;/a>&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;b>Multiprocessing&lt;/b> - multiple CPUs executing concurrently&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Multitasking&lt;/b> - the operating systems simulates concurrency on a single CPU by&lt;br />&#xD;
        interleaving the execution of different tasks&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>Application-based solutions&lt;/b> - the application software takes responsibility for&lt;br />&#xD;
        switching between different branches of code at appropriate times&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Multitasking&quot; name=&quot;Multitasking&quot;>Multitasking&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    When the operating system provides multitasking, a common unit of concurrency is the process. A process is an entity&#xD;
    provided, supported and managed by the operating system whose sole purpose is to provide an environment in which to&#xD;
    execute a program. The process provides a memory space for the exclusive use of its application program, a thread of&#xD;
    execution for executing it, and perhaps some means for sending messages to and receiving them from other processes. In&#xD;
    effect, the process is a virtual CPU for executing a concurrent piece of an application.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Each process has three possible states:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;b>blocked&lt;/b> - waiting to receive some input or gain control of some resource;&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>ready&lt;/b> - waiting for the operating system to give it a turn to execute;&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;b>running&lt;/b> - actually using the CPU.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Processes are also often assigned relative priorities. The operating system kernel determines which process to run at&#xD;
    any given time based upon their states, their priorities, and some scheduling policy. Multitasking operating systems&#xD;
    actually share a single thread of control among all of their processes.&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;example&quot; align=&quot;left&quot;>&#xD;
    &lt;b>Note&lt;/b>: The terms 'task' and 'process' are often used interchangeably. Unfortunately, the term 'multitasking' is&#xD;
    generally used to mean the ability to manage multiple processes at once, while 'multiprocessing' refers to a system&#xD;
    with multiple processors (CPUs). We adhere to this convention because it is the most commonly accepted. However, we use&#xD;
    the term 'task' sparingly, and when we do, it is to make a fine distinction between the unit of work being done (the&#xD;
    task) and the entity which provides the resources and environment for it (the process).&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    As we said before, from the point of view of the CPU, there is only one thread of execution. Just as an application&#xD;
    program can jump from one procedure to another by invoking subroutines, the operating system can transfer control from&#xD;
    one process to another on the occurrence of an interrupt, the completion of a procedure, or some other event. Because&#xD;
    of the memory protection afforded by a process, this &quot;task switching&quot; can carry with it considerable overhead.&#xD;
    Furthermore, because the scheduling policy and process states have little to do with the application viewpoint, the&#xD;
    interleaving of processes is usually too low a level of abstraction for thinking about the kind of concurrency which is&#xD;
    important to the application.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In order to reason clearly about concurrency, it is important to maintain a clear separation between the concept of a&#xD;
    thread of execution and that of task switching. Each process can be thought of as maintaining its own thread of&#xD;
    execution. When the operating system switches between processes, one thread of execution is temporarily interrupted and&#xD;
    another starts or resumes where it previously left off.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Multithreading&quot; name=&quot;Multithreading&quot;>Multithreading&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Many operating systems, particularly those used for real-time applications, offer a &quot;lighter weight&quot; alternative to&#xD;
    processes, called &quot;threads&quot; or &quot;lightweight threads.&quot;&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Threads are a way of achieving a slightly finer granularity of concurrency within a process. Each thread belongs to a&#xD;
    single process, and all the threads in a process share the single memory space and other resources controlled by that&#xD;
    process.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Usually each thread is assigned a procedure to execute.&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;example&quot;>&#xD;
    &lt;b>Note&lt;/b>: It is unfortunate that the term 'threads' is overloaded. When we use the word 'thread' by itself, as we do&#xD;
    here, we are referring to a 'physical thread' provided and managed by the operating system. When we refer to a 'thread&#xD;
    of execution', or 'thread of control' or 'timethread' as in the foregoing discussion, we mean an abstraction which is&#xD;
    not necessarily associated with a physical thread.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Multiprocessing&quot; name=&quot;Multiprocessing&quot;>Multiprocessing&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Of course, multiple processors offer the opportunity for truly concurrent execution. Most commonly, each task is&#xD;
    permanently assigned to a process in a particular processor, but under some circumstances tasks can be dynamically&#xD;
    assigned to the next available processor. Perhaps the most accessible way of doing this is by using a &quot;symmetric&#xD;
    multiprocessor.&quot; In such a hardware configuration, multiple CPUs can access memory through a common bus.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Operating systems which support symmetric multiprocessors can dynamically assign threads to any available CPU. Examples&#xD;
    of operating systems which support symmetric multiprocessors are SUN's Solaris and Microsoft's Windows NT.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Fundamental Issues of Concurrent Software&quot; name=&quot;Fundamental Issues of Concurrent Software&quot;>Fundamental Issues&#xD;
    of Concurrent Software&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Earlier we made the seemingly paradoxical assertions that concurrency both increases and decreases the complexity of&#xD;
    software. Concurrent software provides simpler solutions to complex problems primarily because it permits a &quot;separation&#xD;
    of concerns&quot; among concurrent activities. In this respect, concurrency is just one more tool with which to increase the&#xD;
    modularity of software. When a system must perform predominantly independent activities or respond to predominantly&#xD;
    independent events, assigning them to individual concurrent components naturally simplifies design.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The additional complexities associated with concurrent software arise almost entirely from the situations where these&#xD;
    concurrent activities are almost but not quite independent. In other words, the complexities arise from their&#xD;
    interactions.&amp;nbsp; From a practical standpoint, interactions between asynchronous activities invariably involve the&#xD;
    exchange of some form of signals or information. Interactions between concurrent threads of control give rise to a set&#xD;
    of issues which are unique to concurrent systems, and which must be addressed to guarantee that a system will behave&#xD;
    correctly.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Asynchronous vs. synchronous interaction&quot; name=&quot;Asynchronous vs. synchronous interaction&quot;>Asynchronous vs.&#xD;
    Synchronous Interaction&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Although there are many different specific realizations of inter-process communication (IPC) or inter-thread&#xD;
    communication mechanisms, they can all be ultimately classified into two categories:&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In &lt;b>asynchronous communication&lt;/b> the sending activity forwards its information regardless of whether the receiver&#xD;
    is ready to receive it or not. After launching the information on its way, the sender proceeds with whatever else it&#xD;
    needs to do next. If the receiver is not ready to receive the information, the information is put on some queue where&#xD;
    the receiver can retrieve it later. Both the sender and receiver operate asynchronously of each other, and hence cannot&#xD;
    make assumptions about each other's state. Asynchronous communication is often called &lt;i>message passing&lt;/i>.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;b>Synchronous communication&lt;/b> includes synchronization between the sender and the receiver in addition to the&#xD;
    exchange of information. During the exchange of information, the two concurrent activities merge with each other&#xD;
    executing, in effect, a shared segment of code, and then split up again when the communication is complete. Thus,&#xD;
    during that interval, they are synchronized with each other and immune to concurrency conflicts with each other. If one&#xD;
    activity (sender or receiver) is ready to communicate before the other, it will be suspended until the other one&#xD;
    becomes ready as well. For this reason, this mode of communication is sometimes referred to as &lt;i>rendezvous&lt;/i>.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A potential problem with synchronous communication is that, while waiting on its peer to be ready, an activity is not&#xD;
    capable of reacting to any other events. For many real-time systems, this is not always acceptable because it may not&#xD;
    be possible to guarantee timely response to an important situation. Another drawback is that it is prone to&#xD;
    &lt;i>deadlock&lt;/i>. A deadlock occurs when two or more activities are involved in a vicious circle of waiting on each&#xD;
    other.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    When interactions are necessary between concurrent activities, the designer must choose between a synchronous or&#xD;
    asynchronous style. By synchronous, we mean that two or more concurrent threads of control must rendezvous at a single&#xD;
    point in time. This generally means that one thread of control must wait for another to respond to a request. The&#xD;
    simplest and most common form of synchronous interaction occurs when concurrent activity A requires information from&#xD;
    concurrent activity B in order to proceed with its own work.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Synchronous interactions are, of course, the norm for non-concurrent software components. Ordinary procedure calls are&#xD;
    a prime example of a synchronous interaction: when one procedure calls another, the caller instantaneously transfers&#xD;
    control to the called procedure and effectively &quot;waits&quot; for control to be transferred back to it. In the concurrent&#xD;
    world, however, additional apparatus is needed to synchronize otherwise independent threads of control.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Asynchronous interactions do not require a rendezvous in time, but still require some additional apparatus to support&#xD;
    the communication between two threads of control. Often this apparatus takes the form of communication channels with&#xD;
    message queues so that messages can be sent and received asynchronously.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A&amp;nbsp;single application may mix synchronous and asynchronous communication, depending on whether it needs to wait for&#xD;
    a response or has other work it can do while the message receiver is processing the message.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Keep in mind that true concurrency of processes or threads is only possible on multiprocessors with concurrent&#xD;
    execution of processes or threads; on a uni-processor the illusion of simultaneous execution of threads or processes is&#xD;
    created by the operating system scheduler, which slices the available processing resources into small chunks so that it&#xD;
    appears that several threads or processes are executing concurrently. A poor design will defeat this time slicing by&#xD;
    creating multiple processes or threads which communicate frequently and synchronously, causing processes or threads to&#xD;
    spend much of their &quot;time slice&quot; effectively blocked and waiting for a response from another process or thread.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Contention for shared resources&quot; name=&quot;Contention for shared resources&quot;>Contention for Shared Resources&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Concurrent activities may depend upon scarce resources which must be shared among them. Typical examples are I/O&#xD;
    devices. If an activity requires a resource which is being used by another activity, it must wait its turn.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Race conditions&quot; name=&quot;Race conditions&quot;>Race Conditions: the Issue of Consistent State&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Perhaps the most fundamental issue of concurrent system design is the avoidance of &quot;race conditions.&quot; When part of a&#xD;
    system must perform state-dependent functions (that is, functions whose results depend upon the present state of the&#xD;
    system), it must be assured that that state is stable during the operation. In other words, certain operations must be&#xD;
    &quot;atomic.&quot; Whenever two or more threads of control have access to the same state information, some form of &quot;concurrency&#xD;
    control&quot; is necessary to assure that one thread does not modify the state while the other is performing an atomic&#xD;
    state-dependent operation. Simultaneous attempts to access the same state information which could make the state&#xD;
    internally inconsistent are called &quot;race conditions.&quot;&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A typical example of a race condition could easily occur in the elevator system when a floor is selected by a&#xD;
    passenger. Our elevator works with lists of floors to be visited when traveling in each direction, up and down.&#xD;
    Whenever the elevator arrives at a floor, one thread of control removes that floor from the appropriate list and gets&#xD;
    the next destination from the list. If the list is empty, the elevator either changes direction if the other list&#xD;
    contains floors, or goes idle if both lists are empty. Another thread of control is responsible for putting floor&#xD;
    requests in the appropriate list when the passengers select their floors. Each thread is performing combinations of&#xD;
    operations on the list which are not inherently atomic: for example, checking the next available slot then populating&#xD;
    the slot. If the threads happen to interleave their operations, they can easily overwrite the same slot in the list.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Deadlock&quot; name=&quot;Deadlock&quot;>Deadlock&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Deadlock is a condition in which two threads of control are each blocked, each waiting for the other to take some&#xD;
    action. Ironically, deadlock often arises because we apply some synchronization mechanism to avoid race conditions.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The elevator example of a race condition could easily cause a relatively benign case of deadlock. The elevator control&#xD;
    thread thinks the list is empty and, thus, never visits another floor. The floor request thread thinks the elevator is&#xD;
    working on emptying the list and therefore that it need not notify the elevator to leave the idle state.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Other Practical Issues&quot; name=&quot;Other Practical Issues&quot;>Other Practical Issues&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    In addition to the &quot;fundamental&quot; issues, there are some practical issues which must be explicitly addressed in the&#xD;
    design of concurrent software.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Performance tradeoffs&quot; name=&quot;Performance tradeoffs&quot;>Performance Tradeoffs&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Within a single CPU, the mechanisms required to simulate concurrency by switching between tasks use CPU cycles which&#xD;
    could otherwise be spent on the application itself. On the other hand, if software must wait for I/O devices, for&#xD;
    example, the performance improvements afforded by concurrency may greatly outweigh any added overhead.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Complexity tradeoffs&quot; name=&quot;Complexity tradeoffs&quot;>Complexity Tradeoffs&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Concurrent software requires coordination and control mechanisms not needed in sequential programming applications.&#xD;
    These make concurrent software more complex and increase the opportunities for errors. Problems in concurrent systems&#xD;
    are also inherently more difficult to diagnose because of the multiple threads of control. On the other hand, as we&#xD;
    have pointed out before, when the external driving forces are themselves concurrent, concurrent software which handles&#xD;
    different events independently can be vastly simpler than a sequential program which must accommodate the events in&#xD;
    arbitrary order.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Nondeterminism&quot; name=&quot;Nondeterminism&quot;>Nondeterminism&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Because many factors determine the interleaving of execution of concurrent components, the same software may respond to&#xD;
    the same sequence of events in a different order. Depending upon the design, such changes in order may produce&#xD;
    different results.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;role of application software&quot; name=&quot;role of application software&quot;>The Role of Application Software in&#xD;
    Concurrency Control&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Application software may or may not be involved in the implementation of concurrency control. There is a whole spectrum&#xD;
    of possibilities, including, in order of increasing involvement:&#xD;
&lt;/p>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Application tasks may be interrupted at any time by the operating system (pre-emptive multitasking).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Application tasks may define atomic units of processing (critical sections) which must not be interrupted, and&#xD;
        inform the operating system when they are entered and exited.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Application tasks may decide when to relinquish control of the CPU to other tasks (cooperative multitasking).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Application software may take full responsibility for scheduling and controlling the execution of various tasks.&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;p>&#xD;
    These possibilities are neither an exhaustive set, nor are they mutually exclusive. In a given system a combination of&#xD;
    them may be employed.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Abstracting concurrency&quot; name=&quot;Abstracting concurrency&quot;>Abstracting Concurrency&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    A common mistake in concurrent system design is to select the specific mechanisms to be used for concurrency too early&#xD;
    in the design process. Each mechanism brings with it certain advantages and disadvantages, and the selection of the&#xD;
    &quot;best&quot; mechanism for a particular situation is often determined by subtle trade-offs and compromises. The earlier a&#xD;
    mechanism is chosen, the less information one has upon which to base the selection. Nailing down the mechanism also&#xD;
    tends to reduce the flexibility and adaptability of the design to different situations.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    As with most complex design tasks, concurrency is best understood by employing multiple levels of abstraction. First,&#xD;
    the functional requirements of the system must be well understood in terms of its desired behavior. Next the possible&#xD;
    roles for concurrency should be explored. This is best done using the abstraction of threads without committing to a&#xD;
    particular implementation. To the extent possible, the final selection of mechanisms for realizing the concurrency&#xD;
    should remain open to allow fine tuning of performance and the flexibility to distribute components differently for&#xD;
    various product configurations.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The &quot;conceptual distance&quot; between the problem domain (for example, an elevator system) and the solution domain&#xD;
    (software constructs) remains one of the biggest difficulties in system design. &quot;Visual formalisms&quot; are extremely&#xD;
    helpful for understanding and communicating complex ideas such as concurrent behavior, and, in effect, bridging that&#xD;
    conceptual gap. Among the tools which have proven valuable for such problems are:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        module diagrams for envisioning concurrently acting components;&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        timethreads for envisioning concurrent and interactive activities (which may be orthogonal to the components);&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        sequence diagrams for visualizing interactions between components;&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        state transition diagrams charts for defining the states and state-dependent behaviors of components.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Objects as Concurrent Components&quot; name=&quot;Objects as Concurrent Components&quot;>Objects as Concurrent Components&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    To design a concurrent software system, we must combine the building blocks of software (procedures and data&#xD;
    structures) with the building blocks of concurrency (threads of control). We have discussed the concept of a concurrent&#xD;
    activity, but one doesn't construct systems from activities. One constructs systems from components, and it makes sense&#xD;
    to construct concurrent systems from concurrent components. Taken by themselves, neither procedures nor data structures&#xD;
    nor threads of control make very natural models for concurrent components, but objects seem like a very natural way to&#xD;
    combine all of these necessary elements into one neat package.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    An object packages procedures and data structures into a cohesive component with its own state and behavior. It&#xD;
    encapsulates the specific implementation of that state and behavior and defines an interface by which other objects or&#xD;
    software may interact with it. Objects generally model real world entities or concepts, and interact with other objects&#xD;
    by exchanging messages. They are now well accepted by many as the best way to construct complex systems.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;255&quot; alt=&quot;Diagram is detailed in the content.&quot; src=&quot;./resources/co_cncr4.gif&quot; width=&quot;333&quot; />&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot; align=&quot;center&quot;>&#xD;
    Figure 4:&amp;nbsp;&amp;nbsp; A simple set of objects for the elevator system.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    &lt;br />&#xD;
    Consider an object model for our elevator system. A call station object at each floor monitors the up and down call&#xD;
    buttons at that floor. When a prospective passenger depresses a button, the call station object responds by sending a&#xD;
    message to an elevator dispatcher object, which selects the elevator most likely to provide the fastest service,&#xD;
    dispatches the elevator and acknowledges the call. Each elevator object concurrently and independently controls its&#xD;
    physical elevator counterpart, responding to passenger floor selections and calls from the dispatcher.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Concurrency can take two forms in such an object model. Inter-object concurrency results when two or more objects are&#xD;
    performing activities independently via separate threads of control. Intra-object concurrency arises when multiple&#xD;
    threads of control are active in a single object. In most object-oriented languages today, objects are &quot;passive,&quot;&#xD;
    having no thread of control of their own. The thread(s) of control must be provided by an external environment. Most&#xD;
    commonly, the environment is a standard OS process created to run an object-oriented &quot;program&quot; written in a language&#xD;
    like C++ or Smalltalk. If the OS supports multi-threading, multiple threads can be active in the same or different&#xD;
    objects.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    In the figure below, the passive objects are represented by the circular elements. The shaded interior area of each&#xD;
    object is its state information, and the segmented outer ring is the set of procedures (methods) which define the&#xD;
    object's behavior.&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot; align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;178&quot; alt=&quot;Diagram is detailed in the content.&quot; src=&quot;./resources/co_cncr5.gif&quot; width=&quot;231&quot; />&lt;br />&#xD;
    Figure 5:&amp;nbsp;&amp;nbsp; Illustration of object interaction.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Intra-object concurrency brings with it all of the challenges of concurrent software, such as the potential for race&#xD;
    conditions when multiple threads of control have access to the same memory space-in this case, the data encapsulated in&#xD;
    the object. One might have thought that data encapsulation would provide a solution to this issue. The problem, of&#xD;
    course, is that the object does not encapsulate the thread of control. Although inter-object concurrency avoids these&#xD;
    issues for the most part, there is still one troublesome problem. In order for two concurrent objects to interact by&#xD;
    exchanging messages, at least two threads of control must handle the message and access the same memory space in order&#xD;
    to hand it off. A related (but still more difficult) problem is that of distribution of objects among different&#xD;
    processes or even processors. Messages between objects in different processes requires support for interprocess&#xD;
    communication, and generally require the message to be encoded and decoded into data that can be passed across the&#xD;
    process boundaries.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    None of these problems is insurmountable, of course. In fact, as we pointed out in the previous section, every&#xD;
    concurrent system must deal with them, so there are proven solutions. It is just that &quot;concurrency control&quot; causes&#xD;
    extra work and introduces extra opportunities for error. Furthermore, it obscures the essence of the application&#xD;
    problem. For all of these reasons, we want to minimize the need for application programmers to deal with it explicitly.&#xD;
    One way to accomplish this is to build an object-oriented environment with support for message passing between&#xD;
    concurrent objects (including concurrency control), and minimize or eliminate the use of multiple threads of control&#xD;
    within a single object. In effect, this encapsulates the thread of control along with the data.&#xD;
&lt;/p>&#xD;
&lt;h3 align=&quot;left&quot;>&#xD;
    &lt;a id=&quot;The Active Object model&quot; name=&quot;The Active Object model&quot;>The Active Object Model&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Objects with their own threads of control are called &quot;active objects&quot;. In order to support asynchronous communication&#xD;
    with other active objects, each active object is provided with a message queue or &quot;mailbox.&quot; When an object is created,&#xD;
    the environment gives it its own thread of control, which the object encapsulates until it dies. Like a passive object,&#xD;
    the active object is idle until the arrival of a message from outside. The object executes whatever code is appropriate&#xD;
    to process the message. Any messages which arrive while the object is busy are enqueued in the mailbox. When the object&#xD;
    completes the processing of a message, it returns to pick up the next waiting message in the mailbox, or waits for one&#xD;
    to arrive. Good candidates for active objects in the elevator system include the elevators themselves, the call&#xD;
    stations on each floor, and the dispatcher.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Depending upon their implementation, active objects can be made to be quite efficient. They do carry somewhat more&#xD;
    overhead, however, than a passive object. Thus, since not every operation need be concurrent, it is common to mix&#xD;
    active and passive objects in the same system. Because of their different styles of communication, it is difficult to&#xD;
    make them peers, but an active object makes an ideal environment for passive objects, replacing the OS process we used&#xD;
    earlier. In fact, if the active object delegates all of the work to passive objects, it is basically the equivalent of&#xD;
    an OS process or thread with interprocess communication facilities. More interesting active objects, however, have&#xD;
    behavior of their own to do part of the work, delegating other parts to passive objects.&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot; align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;216&quot; alt=&quot;Diagram is detailed in the content.&quot; src=&quot;./resources/co_cncr6.gif&quot; width=&quot;253&quot; />&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot; align=&quot;center&quot;>&#xD;
    Figure 6:&amp;nbsp;&amp;nbsp; An 'active' object provides an environment for passive classes&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Good candidates for passive objects inside an active elevator object include a list of floors at which the elevator&#xD;
    must stop while going up and another list for going down. The elevator should be able to ask the list for the next&#xD;
    stop, add new stops to the list, and remove stops which have been satisfied.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Because complex systems are almost always constructed of subsystems several levels deep before getting to leaf-level&#xD;
    components, it is a natural extension to the active object model to permit active objects to contain other active&#xD;
    objects.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Although a single-threaded active object does not support true intra-object concurrency, delegating work to contained&#xD;
    active objects is a reasonable substitute for many applications. It retains the important advantage of complete&#xD;
    encapsulation of state, behavior, and thread of control on a per-object basis, which simplifies the concurrency control&#xD;
    issues.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;215&quot; alt=&quot;Diagram is detailed in the content.&quot; src=&quot;./resources/co_cncr7.gif&quot; width=&quot;266&quot; />&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot; align=&quot;center&quot;>&#xD;
    Figure 7:&amp;nbsp;&amp;nbsp; The elevator system, showing nested active objects&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Consider, for example, the partial elevator system depicted above. Each elevator has doors, a hoist, and a control&#xD;
    panel. Each of these components is well-modeled by a concurrent active object, where the door object controls the&#xD;
    opening and closing of the elevator doors, the hoist object controls the positioning of the elevator through the&#xD;
    mechanical hoist, and the control panel object monitors the floor selection buttons and door open/close buttons.&#xD;
    Encapsulating the concurrent threads of control as active objects leads to much simpler software than could be achieved&#xD;
    if all this behavior were managed by a single thread of control.&#xD;
&lt;/p>&#xD;
&lt;h3 align=&quot;left&quot;>&#xD;
    &lt;a id=&quot;The consistent state issue in objects&quot; name=&quot;The consistent state issue in objects&quot;>The 'Consistent State' Issue&#xD;
    in Objects&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    As we said when discussing race conditions, in order for a system to behave in a correct and predictable manner,&#xD;
    certain state-dependent operations must be atomic.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    For an object to behave properly, it is certainly necessary for its state to be internally consistent before and after&#xD;
    processing any message. During the processing of a message, the object's state may be in a transient condition and may&#xD;
    be indeterminate because operations may be only partially complete.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    If an object always completes its response to one message before responding to another, the transient condition is not&#xD;
    a problem. Interrupting one object to execute another also poses no problem because each object practices strict&#xD;
    encapsulation of its state. (Strictly speaking, this is not completely true, as we'll explain soon.)&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Any circumstance under which an object interrupts the processing of a message to process another opens the possibility&#xD;
    of race conditions and, thus, requires the use of concurrency controls. This, in turn, opens the possibility of&#xD;
    deadlock.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Concurrent design is generally simpler, therefore, if objects process each message to completion before accepting&#xD;
    another. This behavior is implicit in the particular form of active object model we have presented.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    The issue of consistent state can manifest itself in two different forms in concurrent systems, and these are perhaps&#xD;
    easier to understand in terms of object-oriented concurrent systems. The first form is that which we have already&#xD;
    discussed. If the state of a single object (passive or active) is accessible to more than one thread of control, atomic&#xD;
    operations must be protected either by the natural atomicity of elementary CPU operations or by a concurrency control&#xD;
    mechanism.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    The second form of the consistent state issue is perhaps more subtle. If more than one object (active or passive)&#xD;
    contains the same state information, the objects will inevitably disagree about the state for at least short intervals&#xD;
    of time.&amp;nbsp; In a poor design they may disagree for longer periods-even forever. This manifestation of inconsistent&#xD;
    state can be considered a mathematical &quot;dual&quot; of the other form.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    For example, the elevator motion control system (the hoist) must assure that the doors are closed and cannot open&#xD;
    before the elevator can move. A design without proper safeguards could permit the doors to open in response to a&#xD;
    passenger hitting the door open button just as the elevator begins to move.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    It may seem that an easy solution to this problem is to permit state information to reside in only one object. Although&#xD;
    this may help, it can also have a detrimental impact on performance, particularly in a distributed system. Furthermore,&#xD;
    it is not a foolproof solution. Even if only one object contains certain state information, as long as other concurrent&#xD;
    objects make decisions based upon that state at a certain point in time, state changes can invalidate the decisions of&#xD;
    other objects.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    There is no magic solution to the problem of consistent state. All practical solutions require us to identify atomic&#xD;
    operations and protect them with some sort of synchronization mechanism which blocks concurrent access for tolerably&#xD;
    short periods of time. &quot;Tolerably short&quot; is very much context dependent. It may be as long as it takes the CPU to store&#xD;
    all the bytes in a floating point number, or it may be as long as it takes the elevator to travel to the next stop.&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
