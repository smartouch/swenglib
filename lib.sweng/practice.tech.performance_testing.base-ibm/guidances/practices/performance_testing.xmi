<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:PracticeDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-gm-e08QJWjoUvI59Qi9lsQ" name="performance_testing,_9iCD4B48Ed2dpYr1Fj8e9Q" guid="-gm-e08QJWjoUvI59Qi9lsQ" changeDate="2008-10-09T06:36:47.062-0700" version="7.5.0">
  <additionalInfo>For more information on this practice,&amp;nbsp;&amp;nbsp;see the &lt;a&#xD;
href=&quot;http://www.ibm.com/developerworks/rational/practices/perf_testing/&quot; target=&quot;_blank&quot;>practice resource page&amp;nbsp;on&#xD;
IBM&amp;reg; DeveloperWorks&amp;reg;&lt;/a>.</additionalInfo>
  <problem>&lt;p> Performance testing is a well-understood discipline that has been practiced &#xD;
  for more than 30 years. First, there were time-sharing capabilities on mainframe &#xD;
  computers, then minicomputers with multiple asynchronous terminals, and later &#xD;
  networks of personal computers connected to server systems. Testing all of these &#xD;
  systems revolved around the need to understand the capacity of the shared portions &#xD;
  of the system. &lt;/p>&#xD;
&lt;p> The process of performance testing has not changed significantly since these &#xD;
  earlier system types were being tested. However, the complexities of the system &#xD;
  design -- with more distributed intelligent hardware components and many more &#xD;
  interconnected software subsystems -- yield more challenges in the analysis &#xD;
  and tuning parts of the process. On current systems, performance testing should &#xD;
  be done iteratively and often during the system design and implementation. Tests &#xD;
  should be performed during implementation of critical subsystems, during their &#xD;
  integration into a complete system, and, finally, under full-capacity workloads &#xD;
  before being deployed into production. &lt;/p></problem>
  <application>&lt;p>&#xD;
    The best way to review a practice is to adopt a multi-prong approach:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Use different perspectives driven by artifacts, activities, test cycles,&amp;nbsp;or roles. Shift between them when&#xD;
        your focus changes from what you need to produce to how or to&amp;nbsp;when an activity is performed.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Start with the performance testing of related &lt;a class=&quot;elementLink&quot;&#xD;
        href=&quot;./../../../practice.tech.performance_testing.base-ibm/customcategories/pt_artifacts_F81D7316.html&quot;&#xD;
        guid=&quot;_6n8fgCKpEd25wpKvxe5l1w&quot;>Artifacts&lt;/a> and decide which ones are important to you and your organization.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Analyze the main&amp;nbsp;&lt;a class=&quot;elementLink&quot;&#xD;
        href=&quot;./../../../practice.tech.performance_testing.base-ibm/capabilitypatterns/Performance%20Testing_A78B2F30.html&quot;&#xD;
        guid=&quot;_98J-UcVLEdy2a8x38u2GrA&quot;>Performance Testing&lt;/a>&amp;nbsp;work pattern, which gives an overview of all of the&#xD;
        activities performed as part of a typical performance testing cycle.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Drill down into each activity to better understand the tasks and artifacts employed.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    The basic steps in the performance testing process are listed here. Each step&amp;nbsp;is captured in the Performance&#xD;
    Testing &lt;a class=&quot;elementLink&quot;&#xD;
    href=&quot;./../../../practice.tech.performance_testing.base-ibm/customcategories/pt_tasks_EE7E89A2.html&quot;&#xD;
    guid=&quot;_ys24QCKpEd25wpKvxe5l1w&quot;>Tasks&lt;/a>.&amp;nbsp;&#xD;
&lt;/p>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Determine the system performance questions that you need to answer.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Characterize the workload that you want to apply to the system.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Identify the important measurements to make within the applied workload.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Establish success criteria for the measurements to be taken.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Design the modeled workload, including elements of variation.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Build the modeled workload elements, and validate each at the various stages of development (single, looped,&#xD;
        parallel, and loaded execution modes).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Construct workload definitions for each of the experiment load levels to collect your workload measurements.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Run the test and monitor the system activities to make sure that the test is running properly.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Analyze measurement results and perform system-tuning activities as necessary to improve the results, and then&#xD;
        repeat test runs as necessary.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Publish the analysis of the measurements to answer the established system performance questions.&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;p>&#xD;
    Also review the guidelines, concepts, and. if applicable, tool-related guidance.&#xD;
&lt;/p></application>
</org.eclipse.epf.uma:PracticeDescription>
