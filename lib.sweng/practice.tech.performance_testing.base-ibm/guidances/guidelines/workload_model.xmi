<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-6qLM8uVO3K4SqnBy4a67ig" name="workload_model,_-1uREODKEdy_TKG9JydNsA" guid="-6qLM8uVO3K4SqnBy4a67ig" changeDate="2008-02-21T14:20:34.656-0800" version="7.2.0">
  <mainDescription>&lt;p>&#xD;
    &lt;strong>Workload Model of the System's Busy Hour&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The workload model usually consists of a characterization of how much work the system must perform at its busiest time.&#xD;
    This represents the peak loaded capacity that the system must support while maintaining satisfactory user visible&#xD;
    characteristics such as responsiveness to user requests for information. The derivation of the workload model is an&#xD;
    important skill expected of a senior systems analyst, a system architect, a business analyst or a performance testing&#xD;
    specialist. However, without a clear analytical workload model, the performance testing project can not truly get&#xD;
    started. Often the performance tester is expected to bring together the business analyst and system architect and build&#xD;
    a workload model that describes the busy hour (or hours) of the developed system.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The workload model may be derived from the number of business transactions expected during the month-end or quarter-end&#xD;
    rush. It might also be based on the late afternoon batch report run interval that overlays an otherwise uniform&#xD;
    transactional workload. It could also be at lunch hour on the Friday after the Thanksgiving holiday for a seasonal&#xD;
    e-commerce Web site.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In most cases, you can think of a workload model as a matrix of transactions (or business scenarios) versus frequency&#xD;
    of execution spread across the number of users accessing the system simultaneously. There are many forms used to&#xD;
    describe this quantity of work. One example is shown below, where the simplified version of the system's load is&#xD;
    reduced to two types of user transactions and two batch reports that are run in the busy hour.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br />&#xD;
&lt;table title=&quot;Example of a Workload Model&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot; width=&quot;85%&quot;&#xD;
summary=&quot;Example of a Workload Model&quot; border=&quot;1&quot;>&#xD;
    &lt;caption>&#xD;
        Example of a Workload Model&#xD;
    &lt;/caption>&#xD;
    &lt;tbody>&#xD;
        &lt;tr>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;col&quot; abbr=&quot;&quot;>&#xD;
                Business Transaction&#xD;
            &lt;/th>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;col&quot; abbr=&quot;&quot;>&#xD;
                System-wide Total/Hour&#xD;
            &lt;/th>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;col&quot; abbr=&quot;&quot;>&#xD;
                Simultaneous Users&#xD;
            &lt;/th>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;col&quot; abbr=&quot;&quot;>&#xD;
                Average Think Time [seconds]&#xD;
            &lt;/th>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;col&quot; abbr=&quot;&quot;>&#xD;
                Records Impacted&#xD;
            &lt;/th>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;row&quot; abbr=&quot;&quot;>&#xD;
                &lt;p>&#xD;
                    Browse Item&#xD;
                &lt;/p>&#xD;
            &lt;/th>&#xD;
            &lt;td>&#xD;
                &lt;p>&#xD;
                    50,000&#xD;
                &lt;/p>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                5,000&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                3.00&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                1&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;row&quot; abbr=&quot;&quot;>&#xD;
                Buy Items&#xD;
            &lt;/th>&#xD;
            &lt;td>&#xD;
                500&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                100&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                5.00&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                3-5&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;row&quot; abbr=&quot;&quot;>&#xD;
                Inventory Report&#xD;
            &lt;/th>&#xD;
            &lt;td>&#xD;
                1&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                1&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                0&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                200,000&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;th id=&quot;&quot; scope=&quot;row&quot; abbr=&quot;&quot;>&#xD;
                Sales Report&#xD;
            &lt;/th>&#xD;
            &lt;td>&#xD;
                1&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                1&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                0&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                8,000&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
    &lt;/tbody>&#xD;
&lt;/table>&lt;br />&#xD;
&lt;br />&#xD;
&lt;p>&#xD;
    The impact on the system across all four components is measured and analyzed. Depending on the system design, any of&#xD;
    the four components could cause significant interactive or batch reporting degradation beyond the minimum acceptable&#xD;
    level of performance expected.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Test Success Criteria&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Ideally the workload model will describe-with a fairly (80-90%) accurate mixture of the transactions (or business&#xD;
    workflows)-the expected peak production workload on the system once it is in use by the business. By liberal&#xD;
    application of the 80/20 rule, the workload model should contain the 20% of the transactions that make up 80% of the&#xD;
    system capacity, and contain most if not all of the crucial business transactions.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Measurement of the throughput of the required transactions is likely to be a minimum standard that must be met by the&#xD;
    system. For example, if during the peak Christmas rush, an online ordering system must process 500 shipments per hour&#xD;
    to meet the projected increase in order volume, based on the previous three years statistics, then the new system must&#xD;
    support this transaction rate.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    On the other hand, if the submit order and order completion screens each take more than 5 seconds to appear, then most&#xD;
    customers will abandon their shopping cart without purchasing, cancel their orders, or not return for subsequent&#xD;
    purchases. These are more customer- oriented requirements that in fact dictate the success of the online shopping site&#xD;
    system deployment.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Both types of requirements must be measured and successfully achieved or there is a significant business risk in going&#xD;
    ahead with system deployment. By helping the system development team focus on these metrics and the underlying causes&#xD;
    of non-compliant transactions or response times, the performance tester can drive a successful outcome of the&#xD;
    performance testing project.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>User Scenarios in the Workload Model&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Business workflows are also considered to be the user scenarios of the system when viewed from the user's perspective.&#xD;
    By walking through the typical user actions that make up each of these business workflows, you can create a user&#xD;
    scenario (also known as a use case in the software requirements world) complete with typical user inputs such as login,&#xD;
    password, product selection(s) for a commerce site or an account number, transaction type, and amount for a financial&#xD;
    site.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The number of user scenarios that make up the workload model is very subjective and can have a major impact on how long&#xD;
    the performance testing project takes. Simplifying the user scenarios and reducing their number have many benefits.&#xD;
    Many times a revision of the application requires that all test scripts be recaptured. This can occur because the&#xD;
    workflow changes in the sequencing of user screens, user input data, or even invisible underlying application parameter&#xD;
    data passed back and forth between browser and server.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    What may seem like a trivial application change from a user flow perspective may cause the test scripts to stop working&#xD;
    properly. If you have only ten user scenarios and a highly productive tool such as Performance Tester, an application&#xD;
    version change can be accommodated in a half day of recapture, minor editing for input data variation, and test&#xD;
    debugging. If you have 20 or more user scenarios, this same event could cost the project a day or more of delay.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Also the number of alternate flows, conditional execution, and more extensive data variation adds to the overall time&#xD;
    to build and maintain the set of tests that implement the workload model. Keeping the user scenarios basic and central&#xD;
    to the important business workflows of the system is an important concept to obtain valuable, timely performance&#xD;
    results for a reasonable project cost. Once you accept the need to abstract the workload definition to a simplified&#xD;
    version of the true user behavior (with virtually equivalent system performance), your performance testing projects&#xD;
    will proceed more quickly and cost effectively.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Input Data Variation in User Scenarios&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Some user inputs, if varied on a per user or per iteration basis, will have an impact on system performance while other&#xD;
    user inputs do not. By reviewing the user scenarios with the system architect, the tester can validate the selection of&#xD;
    which user inputs to vary. By varying the data values in the user scenarios that represent the independent keys of&#xD;
    accessed database tables, representative data retrieval times can be expected. The tester can reduce the possibility of&#xD;
    getting optimistic test results due to database or other subsystem caching effects.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In some cases like the browsing of an online commerce site, repeatedly selecting the same five items to view in the&#xD;
    online catalog may result in the Web server caching images and even product details. However by adding random selection&#xD;
    of the product viewed, the system will experience a much heavier load on the non-Web server subsystems such as an image&#xD;
    server or a backend database containing product details. This difference in the workload may impact order processing&#xD;
    times as well as provide a more realistic estimate on the time to view a product detail page. This may be the critical&#xD;
    customer requirement that determines the success of the Web site.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Workload Model for each Experiment&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    In the performance test planning for the project, the tester designs a series of experiments (or test executions) that&#xD;
    taken together provides a methodical approach to developing the necessary test assets, making necessary test&#xD;
    measurements, permitting the required subsystem and system tuning, making conservative estimates for application&#xD;
    changes and test re-runs, and performing the final measurements and their analysis.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Often there is a requirement for building a subset of the full workload model to provide a means of tuning one or more&#xD;
    of the subsystems, such as tuning the capacity of a single application server normally used in a clustered environment&#xD;
    with a Web server front end. This can be accomplished by only having a single application server active in the cluster&#xD;
    or by redirecting the test connections directly to the application server (bypassing the Web server and load balancer).&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Problems like heap fragmentation, memory leaks, and algorithmic inefficiencies can be more easily be addressed in this&#xD;
    smaller, less complex environment. Tuning the application server including how many instances to run on a hardware&#xD;
    server can be done prior to scaling the system up for a full load test. Some subsystem tuning and testing involves&#xD;
    several subsystems and have to exercised before the full load tests. One example of this is the database connection&#xD;
    pooling to the backend database. By exercising a heavy load of database intensive operations, possibly without&#xD;
    including the entire load of non-database related workload, the database connections from the application servers to&#xD;
    the database can be sized properly.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    As each of these experiments are defined, an associated workload definition should be listed for that experiment. By&#xD;
    pulling these together as part of the test plan, the entire testing project team can understand when the tests and&#xD;
    workload definitions are going to be used and what measurements and data collection needs are expected for each&#xD;
    experiment.&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
