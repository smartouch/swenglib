<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-Rx844shh-oHcd6cpG00h_w" name="new_guideline,_YYAUUODSEdy_TKG9JydNsA" guid="-Rx844shh-oHcd6cpG00h_w" changeDate="2008-02-21T15:14:16.671-0800" version="7.2.0">
  <mainDescription>&lt;p>&#xD;
    Each different experiment has different goals. These goals dictate the size of the workload, its ramp-up period, and&#xD;
    what measurements are taken. For example, if the experiment is to discover the full system capacity with the current&#xD;
    system configuration and tuning, you may try ramping up one virtual user per second in a balanced manner to say two&#xD;
    thirds or three quarters of the system's expected capacity. Once you attain this load level, study the run statistics&#xD;
    for a few minutes at that level to make sure all the transactions are still operating correctly. When this has been&#xD;
    verified (usually by verification point statistics and steady hit rates), manually add another 25% to the workload and&#xD;
    repeat the verification. Add another 25% and perhaps yet another, until you have clearly exceeded some threshold of&#xD;
    acceptability. This may be when response times becoming 2-5 times too long, transaction rates start dropping or staying&#xD;
    stable over a wide load range, or the number of failing verification points become unsatisfactory. Using this kind of&#xD;
    experiment, the verification point, response times, and transaction rates can be examined to determine the maximum&#xD;
    capacity of the system in terms of the number of virtual users.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Another type of experiment is used to delve into why a certain transaction or class of transactions are taking the&#xD;
    system too long to process. After knowing the approximate system capacity, you set up a test run for only a few minutes&#xD;
    of steady state transactions. This test should be run with about a 50% load, but with the application server&#xD;
    instrumentation turned on at the highest level. In addition, all of the servers (Web, application, database, and any&#xD;
    other) key operating system resources should be monitored. Once you have captured the data for the test, you terminate&#xD;
    the test after no more that 10-15 minutes of steady state data. Doing this should ensure that the data is not too&#xD;
    voluminous to analyze. First you filter the data down to only the steady state response interval. Usually by looking at&#xD;
    the user load and the hit rate charts you can discover when a steady state was reached. You can double check this&#xD;
    against the page response time versus time graphs to make sure the response times of interest have become stable. By&#xD;
    decomposing the transaction times into pages and then page elements that are taking the most time to process, you can&#xD;
    see which operations are those most critical to tune. Taking a particular page element, you can then break down by&#xD;
    method call the request that you want to analyze from an application tuning perspective. Depending on the frequency of&#xD;
    calls as well as the time spent in each call, you may want to forward this data to the development organization for&#xD;
    algorithm tuning.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A third type of experiment is used to verify the stability of the system intended for continuous operation without down&#xD;
    time. Many enterprise systems have to be available 24 hours a day / 7 days a week with little if any regular system&#xD;
    down time. To accept a new software version for one of these systems, a 3-7 day test of continuous operation at full&#xD;
    system load is typical to ensure that the system is ready to be put into production. For this test you set up the&#xD;
    statistical results (at the page level only) and operating system monitoring data to be sampled once every 5 to 15&#xD;
    minutes. Test log data is completely turned off for this test. Running this test may require that you write special&#xD;
    logged messages into a data file for every few minutes to verify that transactions are properly executing because&#xD;
    normal logging is turned off. As long as the number of counters being sampled in the statistics model is only a few&#xD;
    hundred, this mode should permit a several day test run. If your test runs with very few errors, you can also run the&#xD;
    test logging set to errors only mode and determine the sampled number of users that you need error data from to get an&#xD;
    accurate picture of a failure mode when it occurs.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The final type of experiment to discuss is that of taking final capacity measurements. In this test run, you will have&#xD;
    resource data being taken from each of your test agent systems to verify that you are in a safe operating region. In&#xD;
    Performance Tester terms, this means that the CPU utilization on the test agents average no more than 70%, and there&#xD;
    are no peak periods where the utilization hits peaks of over 90%. Because the memory allocation is constrained by the&#xD;
    Java heap specified on the test agent, memory statistics should not be an issue. If however you have multiple playback&#xD;
    engines sharing a physical memory that can not easily contain 100% of the Java processes used in its working set, then&#xD;
    you may have to monitor paging and swapping on that test agent. In general, this is not a recommended test agent&#xD;
    configuration because one playback engine can usually consume all available CPU resources on a system. Network and file&#xD;
    I/O data rates should be monitored to make sure that the test agent is not constrained by limited I/O bandwidth and is&#xD;
    unable to accept the server responses at full server output speed.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    If there is a concern about the negative impact of Java garbage collection (GC) on the test agent, you can turn on the&#xD;
    verbose GC logging (using the -verbosegc -verbosegclog:C:\temp\gclog.log) and view the length of time spent doing&#xD;
    garbage collection. There are tools available through the IBM Support Assistant to analyze these logs. In general, this&#xD;
    should not be a problem, unless you are running very close to the edge in heap size and doing frequent, unproductive&#xD;
    garbage collection cycles.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Once you have validated that the test agents are operating in a healthy operating region, you should verify that the&#xD;
    transactions are operating properly and the steady state operating region has been identified and the report data&#xD;
    filtered down to only this data. Based on this region, you can export the performance report and any other reports that&#xD;
    have interesting data so that it can be included in a formal final report of your findings. Candidates include the&#xD;
    transaction report (if those have been defined in the tests), the verification point report, and the HTTP percentile&#xD;
    report (assuming that test log at the primary action level has been kept). The performance report by itself basically&#xD;
    contains all the data necessary to show what the expected page performance of the system is. Statistically speaking&#xD;
    response times are best reported as a 85th, 90th, or 95th percentile while throughput of transactions are best reported&#xD;
    as averages completion times or system rates per second or per hour depending on their execution time.&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
