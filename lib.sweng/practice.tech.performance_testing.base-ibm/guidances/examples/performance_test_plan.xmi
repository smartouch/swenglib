<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:GuidanceDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-yfG8cBbdqeTSXnYIhJ2ubA" name="new_example,_iE-5wOx0Edyf874Kor5UWg" guid="-yfG8cBbdqeTSXnYIhJ2ubA" changeDate="2009-08-05T17:58:52.960-0700" version="7.2.0">
  <mainDescription>&lt;h3>&#xD;
    Introduction&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Online Gardening Supplies is an internet-based outlet selling gardening supplies to the general public. Their web site&#xD;
    generates 100% of their sales and is critical to the survival and success of the business. OGS is re-hosting their&#xD;
    website on IBM Series x346 servers using a three tiered IBM middleware solution that is based on IBM WebSphere&#xD;
    Application Server v6.1, IBM DB2 Database Server, and IBM HTTP Server v6.1.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    This test plan serves as the document that describes the lab equipment required to perform the testing and the&#xD;
    scheduled sequence of experiments required to gather the data called out in the workload specification. These tests are&#xD;
    designed to measure the performance results in the presence of the peak commercial user load on the OGS website after&#xD;
    it is placed into production. Performance of the OGS system under this workload will determine whether the system meets&#xD;
    OGS's performance requirements and whether OGS will accept the system as designed.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Environment&#xD;
&lt;/h3>&#xD;
&lt;h4>&#xD;
    Test Environment&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    The test environment should reflect the production system setup in every way possible. In this case the back-end data&#xD;
    base is currently not the same and does not exhibit the scalability of a separate server running a production IBM DB2&#xD;
    database environment. Without adding this capability to the test system, no formal testing can be performed. Once the&#xD;
    DB2 environment is in place, the system capacity, robustness, and availability testing can be performed.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    System Environment&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    The system under test consists of a front-end IBM HTTP Server v6.1 web server hosted on a dual processor server running&#xD;
    Windows 2003 and an IBM WebSphere Application Server v6.1 hosted on a dual processor server running Windows 2003. In&#xD;
    the production environment, a separate server would be running the database using IBM DB2 Database Server. For these&#xD;
    preliminary tests, the database will reside on the same server as the application server. This will prevent realistic&#xD;
    traffic volumes from being run through this configuration. Performance bottlenecks in the J2EE application code will be&#xD;
    explored prior to deployment of a separate database server. This database deployment will be required to get production&#xD;
    volume traffic and response time estimates can be measured.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The performance testing complex will consist of an RPT console machine and two test agent machines that will be used to&#xD;
    drive the system under test described above. All systems are connected over the 100MB LAN fabric used in the RPT&#xD;
    mini-lab network.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Test Schedule&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The overall test schedule consists of the following test phases:&#xD;
&lt;/p>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Equipment, middleware, and application setup (5 days)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Performance test development (3 days)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Application tuning of J2EE code (5 days)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        System capacity testing (2 days)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Robustness testing (2 days)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Availability testing (10 days)&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;p>&#xD;
    Each of these test phases requires the completion of the previous phase. The database deployment is required for Phases&#xD;
    4 through 6.&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    Equipment and Software Setup&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Installation of all system hardware and networking must be in place. The operating systems, anti-virus and firewall&#xD;
    software then need to be installed. The web server and application server should be installed next. The RPT test agent&#xD;
    software for data collection should then be installed on the web and application servers. The application server should&#xD;
    be instrumented and the data collection monitors started on both servers. Finally a stable version of the application&#xD;
    code should be deployed on the application server. Browser access to the application through the front end web server&#xD;
    completes the sanity testing for the application environment.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The RPT console and test agent systems need to be installed with the latest product version available making sure that&#xD;
    the test agents installed on all systems are the same version as the version of the RPT console. The RPT console server&#xD;
    needs to be licensed for development and point to an IBM Rational license server hosting virtual tester playback&#xD;
    license packs for use during the multi-user testing of the application.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Test lab machines configured with OS, middleware, and application software.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        RPT console and test agent machines configured with latest RPT version.&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    Performance Test Development&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    The performance test scenarios described in the OGS Workload Specification shall be captured using the RPT recorder,&#xD;
    verification points added, and input data variation added. The tests shall be debugged for improper operation in single&#xD;
    execution, looped execution, few user parallel execution, and then in a homogeneous stress load environment. Then these&#xD;
    tests will be declared baselined and ready to be used in a multiple scenario workload environment. Any required&#xD;
    datapools that need to be built should be built and tested as part of this debugging process.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test project&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Fully debugged and parameterized tests for each of the test scenarios&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test schedules used to stress test each individual test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results from individual test playback and stress test playback for each test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Datapool assets to support the required input data variation for each test&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    Application Server Tuning&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    First the ramp-up test is performed to identify the basic application server capacity based on response times and&#xD;
    system resource usage. Then perform the transaction breakdown data collection experiment at two/thirds of capacity.&#xD;
    Analyze the transaction data for any slow transactions and work with development to resolve any unexpected bottlenecks&#xD;
    in the application. Finally perform the one hour steady state test with resource monitoring to make sure the&#xD;
    application performance stays steady. Report any suspected memory leaks back to development for analysis.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test schedule implementing a gradual ramp up to determine application system capacity when response&#xD;
        times become unacceptable&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Performance test schedule used to tune a single application server at two/thirds server capacity with transaction&#xD;
        breakdown tracing turned on&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Performance test schedule used to tune a single application server at full load with resource monitoring configured&#xD;
        for tracking server resources&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results from a gradual ramp up past the full load capacity of the application server to identify full capacity&#xD;
        loading&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results from a 15 minute steady state playback at two/thirds server capacity with transaction breakdown&#xD;
        tracing turned on&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results from a 1 hour steady state playback at full load server capacity with resource monitoring to track&#xD;
        java process size growth to verify there are no memory leaks and that performance stays steady&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    System Capacity Determination&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    On the complete system including the fully deployed and sized database server, run the ramp up schedule to determine&#xD;
    the system capacity full load point using response times and system resource usage. Verify that the test agents can&#xD;
    handle the full system capacity workload and take accurate measurements by verifying agent resource usage. Take the&#xD;
    ramp up measurement well beyond the system capacity whether it is more or less than the expected production workload.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test schedule implementing a gradual ramp up to determine system capacity when response times become&#xD;
        unacceptable&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results with resource monitoring for all servers including the test agents&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    System Capacity Measurement&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Run the workload at the measured system capacity for a period of at least one hour while monitoring system resources to&#xD;
    verify that the system can sustain the capacity found during the ramp-up measurement. Verify that system resources on&#xD;
    the test agents are within satisfactory operating region so the measurements can be validated.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test schedule implementing the maximum system capacity workload&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results with resource monitoring for all servers including the test agents from a steady state run lasting at&#xD;
        least 1 hour&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    Robustness Testing&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Run the workload at 150% of system capacity for at least one hour to prove that you get consistent stable response&#xD;
    times and throughput processing even though the response time is not acceptable. This test is to demonstrate that the&#xD;
    system remains stable even under an overload condition.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test schedule implementing 150% of expected production workload&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results with resource monitoring for all servers including the test agents from a steady state run lasting at&#xD;
        least 1 hour&#xD;
    &lt;/li>&#xD;
&lt;/ol>&#xD;
&lt;h4>&#xD;
    Availability Testing&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Run the system at expected production workload for 72 hours to demonstrate long term stability of the system without&#xD;
    requiring restarts and to show that system processing throughput and response time can be maintained at peak efficiency&#xD;
    for long periods of time.&#xD;
&lt;/p>&#xD;
&lt;h5>&#xD;
    Deliverable Artifacts&#xD;
&lt;/h5>&#xD;
&lt;ol>&#xD;
    &lt;li>&#xD;
        Performance test schedule implementing the expected production workload modified for a 72 hour continuous operation&#xD;
        with reduced data collection&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test results from a steady state run lasting at least 72 hours&#xD;
    &lt;/li>&#xD;
&lt;/ol></mainDescription>
</org.eclipse.epf.uma:GuidanceDescription>
