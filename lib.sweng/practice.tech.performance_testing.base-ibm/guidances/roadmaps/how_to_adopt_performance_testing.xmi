<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-W6UuxtKVcW5fd1q6HAhHOg" name="how_to_adopt_performance_testing,_BVIRUNUbEdySMfcrDUmpxg" guid="-W6UuxtKVcW5fd1q6HAhHOg" changeDate="2008-04-28T14:47:18.562-0700" version="7.2.0">
  <mainDescription>&lt;p> Most reasonably sized IT organizations must establish a &lt;em>performance testing &#xD;
  capability&lt;/em> as one of their standard functions. Taken as an organizational &#xD;
  function, we discuss the constituent parts of building this capability. Beyond &#xD;
  the obvious organizational elements of budget and staff, you need &lt;strong>SWEAT&lt;/strong>: &#xD;
  Steps, Workload, Equipment, Analysis, and Tools. &lt;/p>&#xD;
&lt;p> &lt;strong>Steps&lt;/strong> refers to the need to follow a standard performance &#xD;
  testing process. By obeying all of the important stages in this process, you &#xD;
  can be assured of a reliable conclusion from your performance test. &lt;/p>&#xD;
&lt;p> &lt;strong>Workload&lt;/strong> refers to the user scenarios modeled, rates of execution, &#xD;
  and the identified measurements used in the performance test. Some performance &#xD;
  tests can yield significant results and system improvement by just running a &#xD;
  stress test without an attempt to model the anticipated production workload &#xD;
  on the system. In other cases, where you are trying to ensure that the system &#xD;
  can handle its intended workload, perform the business analysis or distill it &#xD;
  statistically from the current production system if possible. &lt;/p>&#xD;
&lt;p> &lt;strong>Equipment&lt;/strong> refers to the laboratory environment necessary &#xD;
  to implement a full-scale performance test. Often, the system under test is &#xD;
  undersized or the hardware required to drive the test is not anticipated. &lt;/p>&#xD;
&lt;p> &lt;strong>Analysis&lt;/strong> refers to the need to assemble a multi-disciplinary &#xD;
  team, including the system and application architects to participate in performance &#xD;
  testing. They are instrumental in the tuning and subsystem capacity testing, &#xD;
  as well as during the final full-scale testing process. Without their input, &#xD;
  the system tuning would have to be done superficially without the intimate architectural &#xD;
  knowledge of possible optimization,s and it is less likely to result in an acceptable &#xD;
  final result. &lt;/p>&#xD;
&lt;p> &lt;strong>Tools&lt;/strong> refers to the adoption of highly scalable and productive &#xD;
  testing and analysis tools that work with the latest generation of enterprise &#xD;
  servers and their middleware. Today's toolset should include flexible and dynamic &#xD;
  load generation, response time measurement, operating system monitoring, and &#xD;
  distributed application analysis capabilities. Without these functions, today's &#xD;
  highly complex enterprise applications can be difficult to analyze and tune. &#xD;
&lt;/p>&#xD;
&lt;p> Successful performance testing requires a long-term commitment by the organization. &#xD;
  Any enterprise that runs its business on a software-rich infrastructure cannot &#xD;
  only survive, but adapt and thrive by evolving its infrastructure as business &#xD;
  conditions change. Having a capability of repeatedly performance testing new &#xD;
  versions of the software infrastructure is like an insurance policy against &#xD;
  introduction of poor-quality software modifications. This information technology &#xD;
  infrastructure is a strategic asset to businesses competing in today's global &#xD;
  economy. &lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
