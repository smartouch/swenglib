<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="--1u3PetEgKxvVrNe4ig-3w" name="iteration_velocity,_sg-3YOorEdyXraj7Kl_Siw" guid="--1u3PetEgKxvVrNe4ig-3w" authors="Monvorath Phongpaibul" changeDate="2010-09-29T18:19:22.164-0700" version="7.2.0">
  <mainDescription>&lt;h3>&#xD;
    Purpose&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The Iteration Velocity metric is used to measure the capability of the team. It helps identify the trend of how much&#xD;
    work a team can complete in an iteration by tracking how much work it has successfully delivered in past iterations.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Velocity can be used to predict whether the available resources will be able to complete planned tasks within the given&#xD;
    iteration or release. Using a team's average velocity and available resources, you can plot a trend line to determine&#xD;
    when the remaining work will be completed. Factor this in whenever something is added to the scope in the middle of an&#xD;
    iteration.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Definition&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Velocity is typically measured in &lt;em>story points&lt;/em> or &lt;em>ideal days&lt;/em> per iteration. If a team delivered 30&#xD;
    story points in their last iteration, their velocity is 30.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Velocity = Number of units of work that the team has completed during a given iteration. Units can be in points*, ideal&#xD;
    days, days, hours or any unit that the team uses for estimation.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    * Points are units of relative size used in estimating tasks. These can be use case points, feature points, user story&#xD;
    points, stakeholder request points, or Hi-Med-Low complexity. The higher the &quot;points&quot; the more functionality each work&#xD;
    item is worth.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    A good way to monitor Iteration Velocity over time is to use a line or bar chart. The number of points is plotted on&#xD;
    the Y axis, and the iterations are on the X axis.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Analysis&#xD;
&lt;/h3>&#xD;
&lt;h4>&#xD;
    Using Iteration Velocity to monitor project execution&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Iteration Velocity is used as a project execution metric to help a team monitor and steer their project performance. As&#xD;
    velocity is tracked, trends are identified to help the team predict when they can complete the remaining work. If&#xD;
    velocity isn't high enough or stable enough to meet release commitments, the team can take appropriate corrective&#xD;
    action.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Expected trend&lt;/strong> - Ideally, velocity should accelerate over time as the team begins to work better&#xD;
    together and then stabilizes. Velocity can be affected by many factors, such as changes to the team, new tool sets, or&#xD;
    unexpected difficulty in implementing a feature. Barring these types of obstacles, a stable team with required&#xD;
    resources will typically increase their velocity over time, then plateau after three to six iterations. If velocity&#xD;
    goes up or down very quickly there could be a cause for concern to investigate.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Downward slope -&lt;/strong> A trend line that goes down in later iterations is likely due to impending&#xD;
    transition, and could be no cause for concern. However, if the line drops very quickly it could indicate a problem that&#xD;
    is impacting the team's productivity. One possible cause could be quality issues. If the team is spending an increasing&#xD;
    amount of time fixing defects, velocity will drop. The team needs to address the root causes of the poor quality. For&#xD;
    example, the team might need to increase unit tests, bring in an experienced resource to help them with any new&#xD;
    technologies, or introduce pair programming. This trend can be an indicator of a codebase that is not maintainable.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    There are other factors that can contribute to a drop in velocity. Have people left the team? If so, the velocity of&#xD;
    the team will drop and re-scoping might be needed. Has there been a change in communication channels or process that is&#xD;
    impacting the team's ability to complete their work? In these cases, the team might just need a period of time to&#xD;
    adjust to the changes. Otherwise, whatever has changed might need to be reassessed to avoid the need to rescope the&#xD;
    project.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Flat&lt;/strong> - A flat trend line indicates that a team is not increasing their productivity as the project&#xD;
    progresses. Ideally, teams introduce incremental improvements that result in an upward sloping velocity trend line. But&#xD;
    in some cases, factors such as increasing unknowns in the project, or large refactoring efforts cancel out those&#xD;
    incremental improvements. This results in velocity that does not change. Take action to address areas of uncertainty in&#xD;
    the project that are blocking the team from better progress. Confirm that the team morale is high and that motivation&#xD;
    is not an issue.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Up and down&lt;/strong> - A trend line that oscillates up and down, but with no major up or down trends could&#xD;
    indicate that uncompleted work is often carried forward to the next iteration. Because points are not credited for an&#xD;
    iteration unless they are successfully delivered and accepted, some iterations have work credited as completed that was&#xD;
    begun in an earlier iteration. At times, this is due to stakeholder reviews taking too long to complete. Confirm the&#xD;
    review process with stakeholders and emphasize the importance of timely reviews. Otherwise, the team might be&#xD;
    underestimating work. Review estimates in iteration retrospective meetings to determine the cause of the variance.&#xD;
    Confirm that the team is breaking work items down into chunks that are not too large and that they are not estimating&#xD;
    too far out in the future.&lt;br />&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The graph that follows shows a velocity trend in units of points. In initial iterations, the team's velocity is little&#xD;
    low, but it increases and stabilizes as the team proceeds toward the middle iterations.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;img alt=&quot;Iteration Velocity&quot; src=&quot;./resources/iteration_velocity_graph.gif&quot; width=&quot;576&quot; height=&quot;329&quot; />&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    Using Iteration Velocity to monitor capability improvement&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Iteration Velocity is also used as a capability improvement metric metric. It helps a team and middle management&#xD;
    (project manager, product owner) monitor improvements made during the project lifecycle in adopting the Iterative&#xD;
    Development and Release Planning practices. If teams are adopting these practices by timeboxing their iterations,&#xD;
    delivering working software in each iteration, assessing each iteration and making adjustments based on stakeholder&#xD;
    feedback, it will be reflected in their velocity trends. Operational Executives can also use this metric to monitor&#xD;
    systematic improvement in reducing Time to Value across the organization by adopting these practices.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Expected trend&lt;/strong> - Teams adopting Iterative Development and Release Planning will typically accelerate&#xD;
    in velocity over time as the team begins to work better together and understand how to create their solution in&#xD;
    increments. After a few iterations a team's velocity will stabilize.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Velocity at or near zero in early iterations&lt;/strong> - If velocity is extremely low in early iterations, it&#xD;
    could indicate that teams are not successfully adopting iterative development. This trend is seen when teams aren't&#xD;
    doing any actual development (delivering working software) early in the lifecycle. Instead, they are performing in more&#xD;
    of a waterfall fashion, focusing on requirements and design only. Provide a coach or mentor to help teams that are new&#xD;
    to iterative development or that are struggling. Encourage retrospective reviews at the end of each iteration to help&#xD;
    teams evaluate their adoption of iterative development and release planning best practices.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>Up and Down&lt;/strong> - If teams routinely display an Iteration Velocity trend line that oscillates up and down,&#xD;
    they may not be timeboxing their iterations consistently. Re-enforce the construct and benefits of time-boxed iteration&#xD;
    planning and execution. Or, teams may not be receiving timely feedback from stakeholders allowing them to mark work&#xD;
    items as accepted (done) in the iteration in which the work was performed. Involve stakeholders early in the planning&#xD;
    cycle, secure key stakeholder commitments during project schedule review, and communicate the benefits of early&#xD;
    feedback.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Frequency and reporting&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Team members post activity daily in an easily accessible location (common tool, dashboard, teamroom, etc). This&#xD;
    includes work items added and removed, and status updates to work items. The resulting chart can be shown by a team&#xD;
    lead or scrum master during each daily meeting to indicate progress in the iteration, and is reviewed at the end of&#xD;
    each iteration to help identify trends.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Collection and reporting tools&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Iteration Velocity is captured in IBM&amp;reg; Rational&amp;reg; Team Concert&amp;reg;. IBM&amp;reg; Rational&amp;reg; Insight&amp;reg; reports on this metric.&lt;br />&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Assumptions and prerequisites&#xD;
&lt;/h3>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Iterations are scheduled and each team member's role within them established.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The team has a prioritized backlog of work items that is updated daily&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        User stories or use cases are elaborated.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Iteration review meetings take place at the end of each iteration.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The team delivers working software every iteration.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h3>&#xD;
    Pitfalls, advice, and countermeasures&#xD;
&lt;/h3>&#xD;
&lt;div style=&quot;MARGIN-LEFT: 2em&quot;>&#xD;
    &lt;ul>&#xD;
        &lt;li>&#xD;
            When calculating velocity, count only closed work items. Do not count work items that the team partially&#xD;
            completed during the iteration.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Estimate and prioritize defect fixes, but do not count them toward the team's velocity&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Chart team velocity during the whole iteration and for every iteration.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Do not try to predict trends in velocity after only one or two iterations.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            The number of actual hours spent completing a task or a story have no bearing on velocity.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Post big, visible charts in common areas where everyone can see them.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            A cumulative chart is useful, because it shows the total number of work items completed through the end of each&#xD;
            iteration.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Do not compare velocity of one team to another. Each team will estimate their work differently, and teams might&#xD;
            inflate their estimates in order to increase their reported velocity if they know they are being compared to&#xD;
            other teams.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            Track velocity from the beginning of the project, although trends are not as useful in early iterations. If the&#xD;
            project has a long duration, and the team, resources, and technology are all stable, the team may decide to&#xD;
            suspend collecting velocity until circumstances change.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            When a team's velocity trend looks very good, use a Backlog Complexity metric as a countermeasure to confirm&#xD;
            that they are not waiting to address high risk items too late in the lifecycle. Backlog Complexity should be&#xD;
            dropping, but will remain high if the team has a habit of addressing high risk items late in the lifecycle.&#xD;
        &lt;/li>&#xD;
    &lt;/ul>&#xD;
&lt;/div></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
