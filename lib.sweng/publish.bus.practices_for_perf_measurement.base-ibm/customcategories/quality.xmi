<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-Mo3NwPBAcg5Zvcf61WXCog" name="quality,_NrVv4LQqEd6HgZ8EwRW1Og" guid="-Mo3NwPBAcg5Zvcf61WXCog" changeDate="2010-01-13T07:54:40.546-0800" version="7.5.0">
  <mainDescription>&lt;h3>&#xD;
    Overview&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Quality metrics help a team monitor whether or not the solution they are developing will be &quot;good enough&quot; for the client to use successfully in terms of:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        solution reliability.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        completeness.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        meeting its non-functional requirements.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Quality status requires knowing:&#xD;
&lt;/p>&#xD;
&lt;div style=&quot;MARGIN-LEFT: 2em&quot;>&#xD;
    &lt;ul>&#xD;
        &lt;li>&#xD;
            How many defects are there? What is the rate of arrival and closure?&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            What percentage of the requirements have test coverage?&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            How many tests have been planned, implemented (written) and executed?&#xD;
        &lt;/li>&#xD;
    &lt;/ul>&#xD;
&lt;/div>&#xD;
&lt;h3>&#xD;
    Quality metrics&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The following metrics can help a team determine their Quality status:&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    &lt;strong>In-process metrics&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;div dir=&quot;ltr&quot; style=&quot;MARGIN-LEFT: 2em; MARGIN-RIGHT: 0px&quot;>&#xD;
    &lt;ul>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.tech.common.extend_supp_metrics-ibm/guidances/supportingmaterials/defect_density_87F8322D.html&quot; guid=&quot;_Yg1bYITBEd2AfZddKTmDRA&quot;>Defect Density&lt;/a> tracks the number of defects found, fixed, and remaining&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.tech.common.extend_supp_metrics-ibm/guidances/supportingmaterials/defect_trends_2E2A9ABF.html&quot; guid=&quot;_AIQmoITEEd2AfZddKTmDRA&quot;>Defect Trends&lt;/a> help the team monitor the rate of defects arrival and closure&#xD;
            over the project duration.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.tech.common.extend_supp_metrics-ibm/guidances/supportingmaterials/test_execution_status_47109673.html&quot; guid=&quot;_d4L8wCrUEd6FjITr_tW4FQ&quot;>Test Execution Status&lt;/a> helps the team understand overall solution quality in&#xD;
            terms of test completeness and test success.&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.tech.common.extend_supp_metrics-ibm/guidances/supportingmaterials/test_coverage_of_requirements_F7771A44.html&quot; guid=&quot;_IJVvoCthEd6wLLxMlDYvug&quot;>Test Coverage of Requirements&lt;/a> tracks the percentage of requirements that&#xD;
            are covered by tests.&#xD;
        &lt;/li>&#xD;
    &lt;/ul>&#xD;
&lt;/div>&#xD;
&lt;p>&#xD;
    &lt;strong>Post-ship metrics&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;div dir=&quot;ltr&quot; style=&quot;MARGIN-LEFT: 2em; MARGIN-RIGHT: 0px&quot;>&#xD;
    &lt;ul>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.tech.common.extend_supp_metrics-ibm/guidances/supportingmaterials/defect_repair_latency_D729ED8A.html&quot; guid=&quot;_yJtZEKizEd6PSodkFd3_JA&quot;>Defect Repair Latency&lt;/a> tracks the average number of days it takes to repair&#xD;
            critical defects&#xD;
        &lt;/li>&#xD;
        &lt;li>&#xD;
            &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../core.mgmt.common.extend_metrics-ibm/guidances/supportingmaterials/enhancement_request_trend_6560BA36.html&quot; guid=&quot;_vyouZpbmEd27uMM_wDtgBA&quot;>Enhancement Request Trend&lt;/a> monitors requests from stakeholders and&#xD;
            resolution of those requests.&#xD;
        &lt;/li>&#xD;
    &lt;/ul>&#xD;
&lt;/div>&lt;br />&#xD;
&lt;h3>&#xD;
    Reporting project quality status in a dashboard&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The following&amp;nbsp;example dashboard represents one way to calculate and report Quality status. Teams can substitute&#xD;
    metrics that are already in place and used successfully in their organization for a custom Quality report.&lt;br />&#xD;
    &lt;br />&#xD;
    Produce this dashboard by harvesting metrics data and populating a spreadsheet. Generate the dashboard report using&#xD;
    charting capabilities of the spreadsheet tool. Or, the team can use the spreadsheet as a data source and create a&#xD;
    custom report in IBM&amp;reg; Rational&amp;reg; Insight&amp;reg;. Our intent is to provide these types of dashboards over time as out of the&#xD;
    box reports in IBM&amp;reg; Rational&amp;reg; Insight&amp;reg;.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    To assess a project's Quality status with this dashboard, the following inputs are needed (provided by Quality metrics&#xD;
    and team-specific variables):&#xD;
&lt;/p>&#xD;
&lt;table title=&quot;&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot; width=&quot;85%&quot; border=&quot;1&quot;>&#xD;
    &lt;tbody>&#xD;
        &lt;tr>&#xD;
            &lt;td width=&quot;25%&quot; bgcolor=&quot;#CCCCCC&quot;>&#xD;
                &lt;div class=&quot;style1&quot; align=&quot;center&quot;>&#xD;
                    &lt;strong>&lt;font size=&quot;3&quot;>Value&lt;/font>&lt;/strong>&#xD;
                &lt;/div>&#xD;
            &lt;/td>&#xD;
            &lt;td width=&quot;75%&quot; bgcolor=&quot;#CCCCCC&quot;>&#xD;
                &lt;div class=&quot;style1&quot; align=&quot;center&quot;>&#xD;
                    &lt;strong>&lt;font size=&quot;3&quot;>Description&lt;/font>&lt;/strong>&#xD;
                &lt;/div>&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>DEFECT_DENSITY&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Total number of identified defects divided by the size of the system (number of use cases, SLOC, function&#xD;
                points).&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>DEFECT_DENSITY_TARGET&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Maximum Defect Density acceptable for the project.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TEST_COVERAGE_OF_REQUIREMENTS&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Percentage of requirements targeted for a given iteration that have associated test cases.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>MIN_TEST_COVERAGE_OF_REQUIREMENTS&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Minimum Test Coverage of Requirements percentage acceptable for a given iteration.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TESTS_PLANNED&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Number of tests scheduled to be executed in the iteration.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TESTS_IMPLEMENTED&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Number of tests built and ready to be executed ( both manually and automatically) in the iteration.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TESTS_ATTEMPTED&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Number of tests that have been executed. The sum of all passed, failed and blocked tests.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TESTS_FAILED_AND_BLOCKED&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Number of tests that have a most recent result of failed or blocked.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td>&#xD;
                &lt;strong>TESTS_FAILED_AND_BLOCKED_TOLERANCE&lt;/strong>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Acceptable number of tests that have a most recent result of failed or blocked.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
    &lt;/tbody>&#xD;
&lt;/table>&lt;br />&#xD;
&lt;br />&#xD;
&lt;p>&#xD;
    &lt;strong>Quality dashboard status is calculated as follows:&lt;/strong>&#xD;
&lt;/p>&#xD;
&lt;table style=&quot;WIDTH: 944px; HEIGHT: 363px&quot; width=&quot;944&quot; border=&quot;4&quot;>&#xD;
    &lt;tbody>&#xD;
        &lt;tr>&#xD;
            &lt;th scope=&quot;col&quot; width=&quot;99&quot; bgcolor=&quot;#CCCCCC&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;font size=&quot;3&quot;>Status&lt;/font>&#xD;
                &lt;/div>&#xD;
            &lt;/th>&#xD;
            &lt;th scope=&quot;col&quot; width=&quot;421&quot; bgcolor=&quot;#CCCCCC&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;font size=&quot;3&quot;>Definition&lt;/font>&#xD;
                &lt;/div>&#xD;
            &lt;/th>&#xD;
            &lt;th scope=&quot;col&quot; width=&quot;269&quot; bgcolor=&quot;#CCCCCC&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;font size=&quot;3&quot;>Actions to Take&lt;/font>&#xD;
                &lt;/div>&#xD;
            &lt;/th>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td bgcolor=&quot;#ff0000&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;strong>RED&lt;/strong>&#xD;
                &lt;/div>&#xD;
            &lt;/td>&#xD;
            &lt;td bgcolor=&quot;#ffffff&quot;>&#xD;
                &lt;p>&#xD;
                    If any of the following are true:&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        DEFECT_DENSITY &amp;gt; DEFECT_DENSITY_TARGET&#xD;
                    &lt;/li>&#xD;
                    &lt;li>&#xD;
                        TEST COVERAGE_OF_REQUIREMENTS &amp;lt; MIN_TEST_COVERAGE_OF_REQUIREMENTS&#xD;
                    &lt;/li>&#xD;
                    &lt;li>&#xD;
                        TESTS_IMPLEMENTED &amp;lt; TESTS_PLANNED&#xD;
                    &lt;/li>&#xD;
                    &lt;li>&#xD;
                        TESTS_ATTEMPTED &amp;lt; TESTS_PLANNED&#xD;
                    &lt;/li>&#xD;
                    &lt;li>&#xD;
                        TESTS_FAILED_AND_BLOCKED &amp;gt; TESTS_FAILED_AND_BLOCKED_TOLERANCE&lt;br />&#xD;
                        &lt;br />&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
            &lt;/td>&#xD;
            &lt;td bgcolor=&quot;#ffffff&quot;>&#xD;
                &lt;p>&#xD;
                    &lt;strong>Defect Density&lt;/strong>&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        If defect density is greater than the target defect density for the product, perform root cause&#xD;
                        analysis to find the largest contributing schools of defects and adjust the process to prevent&#xD;
                        those schools in the next iteration. Also, repair the defects found (but this will not&#xD;
                        statistically improve quality).&#xD;
                    &lt;/li>&#xD;
                    &lt;li>&#xD;
                        If defect density is increasing iteration to iteration, perform the same analysis as when defect&#xD;
                        density is greater than target, but focus on what new schools of defects are appearing, or any old&#xD;
                        schools that are increasing. If the increase is coming from regression testing, changes are&#xD;
                        breaking code that used to work. Focus on process improvements in architecture or design for the&#xD;
                        next iteration.&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
                &lt;p>&#xD;
                    &lt;strong>Test Coverage of Requirements&lt;/strong>&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        When test coverage is less than the minimum test coverage planned for the iteration, adjust the&#xD;
                        test plan to increase test coverage in the next iteration to sufficiently cover the requirements.&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
                &lt;p>&#xD;
                    &lt;strong>Tests Implemented:&lt;/strong>&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        If the number of Tests Planned is greater than Tests Implemented, the team is over-estimating their&#xD;
                        capability. Adjust the test plan. If Tests Planned is less than Tests Implemented the team has run&#xD;
                        more tests than planned. Look for evidence of Test Scope Creep.&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
                &lt;p>&#xD;
                    &lt;strong>Tests Attempted&lt;/strong>&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        If the number of Tests Attempted is less than Tests Planned, the team is not able to execute as&#xD;
                        many tests as they thought they could. Either the test team is over-estimating their test velocity,&#xD;
                        or the product itself is not testable (e.g. code not complete, environment not available, build not&#xD;
                        stable). If the former condition is true, adjust the test plan. If the latter, a different&#xD;
                        discipline is the source of the under-performance in test. Add to the project risk list and work&#xD;
                        with the team to adjust estimates or fix the problem. For example if Code not Complete is the root,&#xD;
                        raise a risk against the coding velocity and adjust the plan for how much product will be completed&#xD;
                        each iteration. If tests attempted is greater than tests planned, Look for evidence of Test Scope&#xD;
                        Creep.&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
                &lt;p>&#xD;
                    &lt;strong>Tests Failed and Blocked&lt;/strong>&#xD;
                &lt;/p>&#xD;
                &lt;ul>&#xD;
                    &lt;li>&#xD;
                        If tests failed and blocked is greater than the tolerance level set by the team for a given&#xD;
                        iteration, the team needs to identify the reason for the quality issues. If this trend continues,&#xD;
                        the team is not &quot;getting better&quot; at development. The project scope is in jeopardy (date, budget,&#xD;
                        functionality, value). Perform root cause analysis to find the process changes that need to be made&#xD;
                        to get the team back on track. This may be in any of the disciplines including architecture,&#xD;
                        requirements, project management, or code.&#xD;
                    &lt;/li>&#xD;
                &lt;/ul>&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td bgcolor=&quot;#33CC66&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;strong>GREEN&lt;/strong>&#xD;
                &lt;/div>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                &lt;p>&#xD;
                    not red (no conditions for red status are true)&lt;br />&#xD;
                &lt;/p>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                Good job! The team is meeting their quality objectives. Continue to monitor quality dashboard status in&#xD;
                each iteration so that corrective action can be taken if trends change.&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
        &lt;tr>&#xD;
            &lt;td bgcolor=&quot;#ffff00&quot;>&#xD;
                &lt;div align=&quot;center&quot;>&#xD;
                    &lt;strong>YELLOW&lt;/strong>&#xD;
                &lt;/div>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                &lt;br />&#xD;
                &lt;p>&#xD;
                    Status is red, but we have a plan to address the issue.&lt;br />&#xD;
                &lt;/p>&#xD;
            &lt;/td>&#xD;
            &lt;td>&#xD;
                &lt;p>&#xD;
                    Analyze quality metrics to determine the causes for red dashboard status. Determine what actions to&#xD;
                    take and create a plan to address the issues. Follow the plan and monitor progress in each iteration&#xD;
                    until status is green.&#xD;
                &lt;/p>&#xD;
                &lt;p>&#xD;
                    Consult the &lt;a class=&quot;elementLinkWithUserText&quot; href=&quot;./../../publish.bus.practices_for_perf_measurement.base-ibm/customcategories/improve_quality_DA87D734.html&quot; guid=&quot;_0EVR8LbFEd6_i8GY7ieNyA&quot;>Quality Value Traceability Tree&lt;/a> to help identify strategies and&#xD;
                    solutions to improve quality.&lt;br />&#xD;
                &lt;/p>&#xD;
            &lt;/td>&#xD;
        &lt;/tr>&#xD;
    &lt;/tbody>&#xD;
&lt;/table>&lt;br />&#xD;
&lt;br />&#xD;
&lt;h3>&#xD;
    Considerations and referenced terms&#xD;
&lt;/h3>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        High Defect Density is not always a bad thing. It means the team is doing a good job of identifying defects before&#xD;
        the product ships to the customer. The team may need to shift emphasis, however, from defect removal to defect&#xD;
        prevention.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        In general, teams should strive to achieve 100% test coverage. However, in some cases, 80% is enough. The effort&#xD;
        required for that additional 20% of testing may outweigh the benefit of those additional tests. Take the complexity&#xD;
        and scale of your project into account when creating a test coverage target.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Confirm that test coverage includes nonfunctional requirements, not just functional requirements.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Confirm quality objectives with stakeholders when creating the test plan. Have them review the plan to ensure that&#xD;
        it is focused on the right quality issues and priorities.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        If the number of tests planned compared to the team's velocity isn't increasing or stabilizing as iterations are&#xD;
        completed, then the test team has no plan to keep pace with the increasing amount of product that the development&#xD;
        team is able to create. Update iteration test plans to ensure that all new functionality delivered has test&#xD;
        coverage needed to meet quality objectives.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        School of Defect - A set of failures that are caused by the same root cause, such as &quot;uninitialized variables&quot; or&#xD;
        from &quot;missing alternate flows in use cases.&quot; Solving the root will remove multiple defects.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Adjust the Test Plan: This is a pattern for a team to follow when test is under-performing against test needs. When&#xD;
        this occurs, the team will need to add test staff, pursue test automation, or plan for fewer tests and accept the&#xD;
        risk of insufficient test coverage.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test Scope Creep: Sometimes test can over-perform against plan. If the test team is following the process to find&#xD;
        the extra tests, then there is no cause for concern. Increase the tests planned for the next iteration as the test&#xD;
        velocity is higher than the team thought. If the tests are ad-hoc, not traced to requirements, or not following the&#xD;
        process to ensure they are the best tests, this is &quot;Test Scope Creep&quot; and needs to be controlled. Testers are&#xD;
        running what feels right at the time instead of what is optimal based on test team velocity. Note, exploratory test&#xD;
        is still a valid technique. However, the tests run in exploratory testing are not part of the tests planned or&#xD;
        tests implemented measures, so should not affect these measures. Exploratory test should be used and planned for&#xD;
        separately.&lt;br />&#xD;
        &lt;br />&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h3>&#xD;
    Pitfalls and countermeasures&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Quality is only one indicator of project health and success. The team can deliver a product with very few defects, but&#xD;
    if the product is missing needed functionality it will not deliver value to its stakeholders. It is important to&#xD;
    analyze the suggested countermeasures for each core Quality metric to confirm they are driving the correct&#xD;
    behavior.&lt;br />&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
